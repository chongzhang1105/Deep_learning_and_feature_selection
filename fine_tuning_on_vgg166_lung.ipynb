{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine-tuning-on-vgg166_lung.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "afjzwPVNJAC9",
        "YsHVQsNVJAC-",
        "xaDh9Z_DJADm",
        "mMURHESwJAE_"
      ],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGAFgU8bJACU",
        "colab_type": "text"
      },
      "source": [
        "### Building an image classification model using very little data  \n",
        "\n",
        "Based on the tutorial by Francois Chollet @fchollet https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html and the workbook by Guillaume Dominici https://github.com/gggdominici/keras-workshop\n",
        "\n",
        "This tutorial presents several ways to build an image classifier using keras from just a few hundred or thousand pictures from each class you want to be able to recognize.\n",
        "\n",
        "We will go over the following options:  \n",
        "\n",
        "- training a small network from scratch (as a baseline)  \n",
        "- using the bottleneck features of a pre-trained network  \n",
        "- fine-tuning the top layers of a pre-trained network  \n",
        "  \n",
        "This will lead us to cover the following Keras features:   \n",
        "  \n",
        "- fit_generator for training Keras a model using Python data generators  \n",
        "- ImageDataGenerator for real-time data augmentation  \n",
        "- layer freezing and model fine-tuning  \n",
        "- ...and more.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeHEBWR0JACW",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fMI-_hxJACW",
        "colab_type": "text"
      },
      "source": [
        "Data can be downloaded at:\n",
        "https://www.kaggle.com/c/dogs-vs-cats/data  \n",
        "All you need is the train set  \n",
        "The recommended folder structure is:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8e2VN_cJACX",
        "colab_type": "text"
      },
      "source": [
        "### Folder structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoRGEnDEJACY",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "data/\n",
        "    train/\n",
        "        dogs/ ### 1024 pictures\n",
        "            dog001.jpg\n",
        "            dog002.jpg\n",
        "            ...\n",
        "        cats/ ### 1024 pictures\n",
        "            cat001.jpg\n",
        "            cat002.jpg\n",
        "            ...\n",
        "    validation/\n",
        "        dogs/ ### 416 pictures\n",
        "            dog001.jpg\n",
        "            dog002.jpg\n",
        "            ...\n",
        "        cats/ ### 416 pictures\n",
        "            cat001.jpg\n",
        "            cat002.jpg\n",
        "            ...\n",
        "```\n",
        "Note : for this example we only consider 2x1000 training images and 2x400 testing images among the 2x12500 available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME1aMVoLJACY",
        "colab_type": "text"
      },
      "source": [
        "The github repo includes about 1500 images for this model. The original Kaggle dataset is much larger. The purpose of this demo is to show how you can build models with smaller size datasets. You should be able to improve this model by using more data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oevEPKhJJACZ",
        "colab_type": "text"
      },
      "source": [
        "### Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mo4VG2hJFPx",
        "colab_type": "code",
        "outputId": "6a95943d-d446-4c6f-aae9-9262ed2e6b04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNXgd-3czMuM",
        "colab_type": "code",
        "outputId": "2ab1636a-dcd5-4912-e22c-ada60d69221b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May 20 19:52:12 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8    17W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ur6oZQb0GWx",
        "colab_type": "code",
        "outputId": "3be27d73-044e-4398-e82a-df50adbb43e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 11925368468887995304, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 16238337610764221340\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 16664474100595696373\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14800692839\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 8255418864457878577\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8IKDTHHJACf",
        "colab_type": "code",
        "outputId": "b5155efd-8d41-4591-9945-e18dfe3cd3dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##Updated to Keras 2.0\n",
        "import os\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras import optimizers\n",
        "from keras import applications\n",
        "from keras.models import Model\n",
        "from IPython.display import Image\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEoG6ng1PSaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR5qZknrJACm",
        "colab_type": "text"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL9VPww2JACn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_width, img_height = 128, 128\n",
        "batch_size = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjgpMAcHQgdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_array=np.load('/content/drive/My Drive/gpu/3direction_array_intrain.npy',allow_pickle=True)\n",
        "inval_array=np.load('/content/drive/My Drive/gpu/3direction_array_inval.npy',allow_pickle=True)\n",
        "val_array=np.load('/content/drive/My Drive/gpu/3direction_array_val.npy',allow_pickle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDK2fJVDpXf8",
        "colab_type": "code",
        "outputId": "0d5bfa1e-d995-4ab3-a71c-af4e3c083f54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_array.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35569, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ry-CFTw12_L",
        "colab_type": "text"
      },
      "source": [
        "###train data processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOUBno2Ci0fK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=np.array([i[2] for i in train_array])\n",
        "train_label=np.array([i[1] for i in train_array])\n",
        "train_id=np.array([i[0] for i in train_array])\n",
        "train_data = train_data.reshape(-1, 128,128, 1)\n",
        "train_data=train_data.astype('float32')\n",
        "train_data = train_data / np.max(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp7RmrCl0agY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_id_unique=np.unique(train_id)\n",
        "train_id_label=np.array(list(zip(train_id,train_label)))\n",
        "train_unique_labels=[]\n",
        "for i in range(len(train_id_unique)):\n",
        "  ID=train_id_unique[i]\n",
        "  index=(train_id_label[:,0]==ID)\n",
        "  label=int(np.unique(train_id_label[index][:,1])[0])\n",
        "  train_unique_labels.append(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2_P2s6H167N",
        "colab_type": "text"
      },
      "source": [
        "###inval data processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaP7Glvgp3GZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inval_data=np.array([i[2] for i in inval_array])\n",
        "inval_label=np.array([i[1] for i in inval_array])\n",
        "inval_id=np.array([i[0] for i in inval_array])\n",
        "inval_data = inval_data.reshape(-1, 128,128, 1)\n",
        "inval_data=inval_data.astype('float32')\n",
        "inval_data = inval_data / np.max(inval_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjC3A9_I2J2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inval_id_unique=np.unique(inval_id)\n",
        "inval_id_label=np.array(list(zip(inval_id,inval_label)))\n",
        "inval_unique_labels=[]\n",
        "for i in range(len(inval_id_unique)):\n",
        "  ID=inval_id_unique[i]\n",
        "  index=(inval_id_label[:,0]==ID)\n",
        "  label=int(np.unique(inval_id_label[index][:,1])[0])\n",
        "  inval_unique_labels.append(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daCuxr2l2b6V",
        "colab_type": "text"
      },
      "source": [
        "## val data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb1ACg3iqSBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data=np.array([i[2] for i in val_array])\n",
        "val_label=np.array([i[1] for i in val_array])\n",
        "val_id=np.array([i[0] for i in val_array])\n",
        "val_data = val_data.reshape(-1, 128,128, 1)\n",
        "val_data=val_data.astype('float32')\n",
        "val_data = val_data / np.max(val_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDCryNg00cG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_id_unique=np.unique(val_id)\n",
        "val_id_label=np.array(list(zip(val_id,val_label)))\n",
        "val_unique_labels=[]\n",
        "for i in range(len(val_id_unique)):\n",
        "  ID=val_id_unique[i]\n",
        "  index=(val_id_label[:,0]==ID)\n",
        "  label=int(np.unique(val_id_label[index][:,1])[0])\n",
        "  val_unique_labels.append(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN9KWoYeztw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "train_label1=to_categorical(train_label,2)\n",
        "inval_label1=to_categorical(inval_label,2)\n",
        "val_label1=to_categorical(val_label,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUp_eyQiuUWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(len(all_id_unique)):\n",
        "#   ID=all_id_unique[i]\n",
        "#   index=(id_prediction[:,0]==ID)\n",
        "#   prediction=list(id_prediction[index][:,1])\n",
        "#   if prediction.count('1')>=prediction.count('0'):\n",
        "#     final_pre=1\n",
        "#   else:\n",
        "#     final_pre=0\n",
        "#   unique_predictions.append(final_pre)\n",
        "#   mix=[[ID],prediction]\n",
        "#   combine_data.append(mix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lBEEbVoxnyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_id_label=np.array(list(zip(test_id,test_label)))\n",
        "# test_id_unique=np.unique(test_id)\n",
        "# test_unique_labels=[]\n",
        "# for i in range(len(test_id_unique)):\n",
        "#   ID=test_id_unique[i]\n",
        "#   index=(test_id_label[:,0]==ID)\n",
        "#   label=int(np.unique((test_id_label[index][:,1])))\n",
        "#   print(ID)\n",
        "# #   test_unique_labels.append(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij9Ns_DIV8-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RId8Kj0JACp",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byyKLMF0JACq",
        "colab_type": "code",
        "outputId": "b1ba6dae-80a7-42bb-fc58-e50540084c32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "##preprocessing\n",
        "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 8\n",
        "\n",
        "# automagically retrieve images and their classes for train and validation sets\n",
        "train_generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        color_mode='grayscale',\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        shuffle=False)\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        color_mode='grayscale',\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5502 images belonging to 2 classes.\n",
            "Found 1835 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux6g0OdbJACt",
        "colab_type": "text"
      },
      "source": [
        "## Small Conv Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwdVcAOaJACu",
        "colab_type": "text"
      },
      "source": [
        "### Model architecture definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn_qcVR1JACv",
        "colab_type": "code",
        "outputId": "3d8de061-d307-484a-d82e-017799657c54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers.\n",
        "model = Sequential()\n",
        "model.add(Convolution2D(32, (3, 3), input_shape=(img_width, img_height,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Convolution2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Convolution2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM7BI8MAty0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers.normalization import BatchNormalization\n",
        "model = Sequential()\n",
        "model.add(Convolution2D(32, (3, 3), input_shape=(img_width, img_height,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Convolution2D(32, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Convolution2D(64, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKCuS7GL9BRc",
        "colab_type": "code",
        "outputId": "f9a7ae62-1adc-4a9d-8854-844e2e1d9d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(ZeroPadding2D((1,1),input_shape=(128,128,1)))\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(2,2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuWPKrSCZTyv",
        "colab_type": "code",
        "outputId": "cf0f717f-c8f8-446f-ced0-3cbde427db27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 126, 126, 32)      320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 126, 126, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 126, 126, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 63, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 61, 61, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 61, 61, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 61, 61, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 28, 28, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                802880    \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 2)                 130       \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 831,586\n",
            "Trainable params: 831,330\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu9mxjAsMoDj",
        "colab_type": "code",
        "outputId": "f550d97f-82bd-4828-cf8b-fa3d4670c0cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.layers[0].input"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'conv2d_1_input:0' shape=(?, 128, 128, 1) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gVBCjUqJACx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "sgd=optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIZBpUHjJACy",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPKVqt229AtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_gyMUj2JACz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 300\n",
        "batch_size=50\n",
        "train_samples = 4924\n",
        "validation_samples = 1231\n",
        "stop=keras.callbacks.EarlyStopping(monitor='val_loss',patience=50,mode='min', verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhaae_vbXsVC",
        "colab_type": "code",
        "outputId": "9735616e-b0f5-468c-8841-88a12ee38227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1805
        }
      },
      "source": [
        "history=model.fit(train_data,train_label1,validation_data=(inval_data,inval_label1),callbacks=[stop],epochs=50,verbose=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 35569 samples, validate on 7861 samples\n",
            "Epoch 1/50\n",
            "35569/35569 [==============================] - 35s 988us/step - loss: 0.6392 - acc: 0.6569 - val_loss: 0.6507 - val_acc: 0.6442\n",
            "Epoch 2/50\n",
            "35569/35569 [==============================] - 31s 883us/step - loss: 0.6233 - acc: 0.6611 - val_loss: 0.6500 - val_acc: 0.6432\n",
            "Epoch 3/50\n",
            "35569/35569 [==============================] - 31s 875us/step - loss: 0.5817 - acc: 0.6907 - val_loss: 0.6561 - val_acc: 0.6371\n",
            "Epoch 4/50\n",
            "35569/35569 [==============================] - 31s 885us/step - loss: 0.5208 - acc: 0.7395 - val_loss: 0.6450 - val_acc: 0.6270\n",
            "Epoch 5/50\n",
            "35569/35569 [==============================] - 31s 876us/step - loss: 0.4507 - acc: 0.7837 - val_loss: 0.6630 - val_acc: 0.6266\n",
            "Epoch 6/50\n",
            "35569/35569 [==============================] - 31s 876us/step - loss: 0.3908 - acc: 0.8177 - val_loss: 0.7045 - val_acc: 0.6124\n",
            "Epoch 7/50\n",
            "35569/35569 [==============================] - 31s 884us/step - loss: 0.3420 - acc: 0.8431 - val_loss: 0.7350 - val_acc: 0.6181\n",
            "Epoch 8/50\n",
            "35569/35569 [==============================] - 31s 880us/step - loss: 0.3086 - acc: 0.8601 - val_loss: 0.7372 - val_acc: 0.6132\n",
            "Epoch 9/50\n",
            "35569/35569 [==============================] - 32s 886us/step - loss: 0.2770 - acc: 0.8776 - val_loss: 0.7472 - val_acc: 0.6175\n",
            "Epoch 10/50\n",
            "35569/35569 [==============================] - 31s 874us/step - loss: 0.2530 - acc: 0.8862 - val_loss: 0.8073 - val_acc: 0.6011\n",
            "Epoch 11/50\n",
            "35569/35569 [==============================] - 31s 872us/step - loss: 0.2334 - acc: 0.8954 - val_loss: 0.8986 - val_acc: 0.6144\n",
            "Epoch 12/50\n",
            "35569/35569 [==============================] - 31s 881us/step - loss: 0.2151 - acc: 0.9058 - val_loss: 0.8481 - val_acc: 0.6109\n",
            "Epoch 13/50\n",
            "35569/35569 [==============================] - 31s 875us/step - loss: 0.1964 - acc: 0.9140 - val_loss: 0.9107 - val_acc: 0.6129\n",
            "Epoch 14/50\n",
            "35569/35569 [==============================] - 32s 891us/step - loss: 0.1926 - acc: 0.9158 - val_loss: 0.9104 - val_acc: 0.6091\n",
            "Epoch 15/50\n",
            "35569/35569 [==============================] - 31s 877us/step - loss: 0.1830 - acc: 0.9200 - val_loss: 0.9044 - val_acc: 0.6002\n",
            "Epoch 16/50\n",
            "35569/35569 [==============================] - 31s 875us/step - loss: 0.1709 - acc: 0.9249 - val_loss: 0.9292 - val_acc: 0.5990\n",
            "Epoch 17/50\n",
            "35569/35569 [==============================] - 31s 882us/step - loss: 0.1631 - acc: 0.9282 - val_loss: 1.0092 - val_acc: 0.6199\n",
            "Epoch 18/50\n",
            "35569/35569 [==============================] - 31s 874us/step - loss: 0.1594 - acc: 0.9303 - val_loss: 0.9953 - val_acc: 0.6002\n",
            "Epoch 19/50\n",
            "35569/35569 [==============================] - 31s 880us/step - loss: 0.1502 - acc: 0.9345 - val_loss: 1.0275 - val_acc: 0.6006\n",
            "Epoch 20/50\n",
            "35569/35569 [==============================] - 31s 878us/step - loss: 0.1453 - acc: 0.9365 - val_loss: 0.9743 - val_acc: 0.5993\n",
            "Epoch 21/50\n",
            "35569/35569 [==============================] - 31s 874us/step - loss: 0.1419 - acc: 0.9372 - val_loss: 1.0472 - val_acc: 0.6042\n",
            "Epoch 22/50\n",
            "35569/35569 [==============================] - 31s 881us/step - loss: 0.1413 - acc: 0.9375 - val_loss: 1.0328 - val_acc: 0.6090\n",
            "Epoch 23/50\n",
            "35569/35569 [==============================] - 31s 873us/step - loss: 0.1312 - acc: 0.9434 - val_loss: 1.1889 - val_acc: 0.5987\n",
            "Epoch 24/50\n",
            "35569/35569 [==============================] - 31s 882us/step - loss: 0.1262 - acc: 0.9456 - val_loss: 1.0981 - val_acc: 0.5956\n",
            "Epoch 25/50\n",
            "35569/35569 [==============================] - 31s 879us/step - loss: 0.1215 - acc: 0.9456 - val_loss: 1.1505 - val_acc: 0.5981\n",
            "Epoch 26/50\n",
            "35569/35569 [==============================] - 31s 874us/step - loss: 0.1177 - acc: 0.9486 - val_loss: 1.1995 - val_acc: 0.6025\n",
            "Epoch 27/50\n",
            "35569/35569 [==============================] - 31s 880us/step - loss: 0.1156 - acc: 0.9481 - val_loss: 1.2487 - val_acc: 0.6054\n",
            "Epoch 28/50\n",
            "35569/35569 [==============================] - 31s 873us/step - loss: 0.1091 - acc: 0.9525 - val_loss: 1.2613 - val_acc: 0.5995\n",
            "Epoch 29/50\n",
            "35569/35569 [==============================] - 31s 878us/step - loss: 0.1129 - acc: 0.9502 - val_loss: 1.1683 - val_acc: 0.5828\n",
            "Epoch 30/50\n",
            "35569/35569 [==============================] - 31s 880us/step - loss: 0.1057 - acc: 0.9543 - val_loss: 1.1282 - val_acc: 0.6054\n",
            "Epoch 31/50\n",
            "35569/35569 [==============================] - 31s 872us/step - loss: 0.1021 - acc: 0.9535 - val_loss: 1.2663 - val_acc: 0.5955\n",
            "Epoch 32/50\n",
            "35569/35569 [==============================] - 31s 878us/step - loss: 0.1027 - acc: 0.9556 - val_loss: 1.2498 - val_acc: 0.5911\n",
            "Epoch 33/50\n",
            "35569/35569 [==============================] - 31s 874us/step - loss: 0.0978 - acc: 0.9581 - val_loss: 1.2931 - val_acc: 0.6065\n",
            "Epoch 34/50\n",
            "35569/35569 [==============================] - 31s 883us/step - loss: 0.1003 - acc: 0.9558 - val_loss: 1.2826 - val_acc: 0.5994\n",
            "Epoch 35/50\n",
            "35569/35569 [==============================] - 31s 879us/step - loss: 0.0925 - acc: 0.9596 - val_loss: 1.3515 - val_acc: 0.5908\n",
            "Epoch 36/50\n",
            "35569/35569 [==============================] - 31s 873us/step - loss: 0.0958 - acc: 0.9586 - val_loss: 1.3938 - val_acc: 0.5899\n",
            "Epoch 37/50\n",
            "35569/35569 [==============================] - 31s 876us/step - loss: 0.0923 - acc: 0.9595 - val_loss: 1.3328 - val_acc: 0.5914\n",
            "Epoch 38/50\n",
            "35569/35569 [==============================] - 31s 878us/step - loss: 0.0924 - acc: 0.9602 - val_loss: 1.4074 - val_acc: 0.5925\n",
            "Epoch 39/50\n",
            "35569/35569 [==============================] - 31s 878us/step - loss: 0.0875 - acc: 0.9615 - val_loss: 1.3110 - val_acc: 0.6087\n",
            "Epoch 40/50\n",
            "35569/35569 [==============================] - 31s 880us/step - loss: 0.0914 - acc: 0.9599 - val_loss: 1.3691 - val_acc: 0.5962\n",
            "Epoch 41/50\n",
            "35569/35569 [==============================] - 31s 873us/step - loss: 0.0850 - acc: 0.9637 - val_loss: 1.3290 - val_acc: 0.5928\n",
            "Epoch 42/50\n",
            "35569/35569 [==============================] - 31s 875us/step - loss: 0.0847 - acc: 0.9616 - val_loss: 1.4556 - val_acc: 0.5922\n",
            "Epoch 43/50\n",
            "35569/35569 [==============================] - 31s 882us/step - loss: 0.0821 - acc: 0.9625 - val_loss: 1.4297 - val_acc: 0.6011\n",
            "Epoch 44/50\n",
            "35569/35569 [==============================] - 31s 885us/step - loss: 0.0832 - acc: 0.9624 - val_loss: 1.3765 - val_acc: 0.5956\n",
            "Epoch 45/50\n",
            "35569/35569 [==============================] - 31s 881us/step - loss: 0.0812 - acc: 0.9658 - val_loss: 1.3988 - val_acc: 0.5886\n",
            "Epoch 46/50\n",
            "35569/35569 [==============================] - 31s 876us/step - loss: 0.0825 - acc: 0.9647 - val_loss: 1.5402 - val_acc: 0.6004\n",
            "Epoch 47/50\n",
            "35569/35569 [==============================] - 31s 874us/step - loss: 0.0812 - acc: 0.9642 - val_loss: 1.6002 - val_acc: 0.5937\n",
            "Epoch 48/50\n",
            "35569/35569 [==============================] - 31s 885us/step - loss: 0.0774 - acc: 0.9664 - val_loss: 1.5440 - val_acc: 0.5934\n",
            "Epoch 49/50\n",
            "35569/35569 [==============================] - 31s 873us/step - loss: 0.0791 - acc: 0.9655 - val_loss: 1.4615 - val_acc: 0.5889\n",
            "Epoch 50/50\n",
            "35569/35569 [==============================] - 31s 878us/step - loss: 0.0755 - acc: 0.9666 - val_loss: 1.4843 - val_acc: 0.5925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v_bQ9jKVZJq",
        "colab_type": "code",
        "outputId": "a8df64d9-ece8-46cd-b961-9923fc46607f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTHQKIYQQMtEphBBCyESnEEIIIfMfBN4vJ/WrF/MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE1PevtdNCgwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrRv2VACJAC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit_generator(\n",
        "#         train_generator,\n",
        "#         steps_per_epoch=train_samples // batch_size,\n",
        "#         callbacks=[stop],\n",
        "#         epochs=epochs,\n",
        "#         validation_data=validation_generator,\n",
        "#         validation_steps=validation_samples// batch_size,)\n",
        "# #About 60 seconds an epoch when using CPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4hqIE9BJAC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('/content/drive/My Drive/gpu/top_model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cndZ8OhyJAC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.save_weights('models_trained/basic_cnn_30_epochs.h5')\n",
        "model.load_weights('models_trained/basic_cnn_30_epochs.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skd-bLqOJAC8",
        "colab_type": "text"
      },
      "source": [
        "If your model successfully runs at one epoch, go back and it for 30 epochs by changing nb_epoch above.  I was able to get to an val_acc of 0.71 at 30 epochs.\n",
        "A copy of a pretrained network is available in the pretrained folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afjzwPVNJAC9",
        "colab_type": "text"
      },
      "source": [
        "### Validating the Model's Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsHVQsNVJAC-",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing a Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3wkhcA0JAC-",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAvAFWMeJAC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using quiver\n",
        "#!pip install quiver_engine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StVQf53sJADC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "model_vgg = VGG16(weights='imagenet', include_top=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5LhiGWNJADK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from quiver_engine import server\n",
        "#server.launch(model_vgg,input_folder='./raj')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAENenaZJADL",
        "colab_type": "text"
      },
      "source": [
        "Computing loss and accuracy :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNw2rJdipTF8",
        "colab_type": "code",
        "outputId": "4a5c0eee-6c81-4f22-d249-8a9c3a2165c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(train_data,train_label1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4924/4924 [==============================] - 1s 204us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.21723145989733872, 0.9555239642082822]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Q08rOVJADM",
        "colab_type": "code",
        "outputId": "6b8a3b24-50dd-49c0-e61b-eda8db1a26e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(test_data,test_label1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1231/1231 [==============================] - 0s 254us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9565536082033023, 0.7912266451493208]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvosSnpQz_Vb",
        "colab_type": "code",
        "outputId": "ae812c37-235c-4b92-ae26-0c5683569dab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(validation_data,validation_label1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1182/1182 [==============================] - 0s 200us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.3569854887205532, 0.6032148899160666]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLlcVwyL0Rpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H5epbBB9PFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_prob=model.predict(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E5zz-h1fWXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_classes=model.predict_classes(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C78Xa_kL-XC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_classes = model.predict_classes(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsToND1abydt",
        "colab_type": "code",
        "outputId": "697d99a5-3bb8-4f17-b6a6-15dc9dd7ddfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrVlSn0iAjCR",
        "colab_type": "code",
        "outputId": "4ff6f6d3-b535-44a1-ec32-18282636c01d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(test_classes).count(0),list(test_classes).count(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(822, 371)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw3OiCUiAv5B",
        "colab_type": "code",
        "outputId": "f2ea86d6-c77e-44b5-a48f-b2fcae98aa46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(test_label).count(0),list(test_label).count(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(781, 412)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1n4O5u0lX_",
        "colab_type": "text"
      },
      "source": [
        "**validation performance check**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N89jDSFw0plj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_prob=model.predict(validation_data)\n",
        "validation_classes=model.predict_classes(validation_data)\n",
        "validation_preds=np.argmax(validation_prob,axis=1)\n",
        "validation_prob=validation_prob[:,1]\n",
        "val_id_unique=np.unique(validation_id)\n",
        "val_id_prediction=np.array(list(zip(validation_id,validation_preds)))\n",
        "val_combine_data=[]\n",
        "val_unique_predictions=[]\n",
        "for i in range(len(val_id_unique)):\n",
        "  ID=val_id_unique[i]\n",
        "  index=(val_id_prediction[:,0]==ID)\n",
        "  prediction=list(val_id_prediction[index][:,1])\n",
        "  if prediction.count('1')>=prediction.count('0'):\n",
        "    final_pre=1\n",
        "  else:\n",
        "    final_pre=0\n",
        "  val_unique_predictions.append(final_pre)\n",
        "  mix=[[ID],prediction]\n",
        "  val_combine_data.append(mix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5Qg1ZTz2ZmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_id_label=np.array(list(zip(validation_id,validation_label)))\n",
        "val_unique_labels=[]\n",
        "for i in range(len(val_id_unique)):\n",
        "  ID=val_id_unique[i]\n",
        "  index=(val_id_label[:,0]==ID)\n",
        "  label=int(np.unique(val_id_label[index][:,1])[0])\n",
        "  val_unique_labels.append(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIY3AWBodvlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6_s2KrkbCkL",
        "colab_type": "code",
        "outputId": "18194761-0308-4b5b-b610-149b08418325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(val_unique_labels,val_unique_predictions)\n",
        "print('confusion matrix \\n',cm)\n",
        "total=sum(sum(cm))\n",
        "accuracy=(cm[0,0]+cm[1,1])/total\n",
        "print('accuray ',accuracy)\n",
        "sensitivity=cm[1,1]/(cm[1,1]+cm[1,0])\n",
        "print('sensitivity ',sensitivity)\n",
        "specificity=cm[0,0]/(cm[0,0]+cm[0,1])\n",
        "print('specificity ',specificity)\n",
        "ppv = cm[1,1]/(cm[1,1]+cm[0,1])\n",
        "print('PPV  ', ppv)\n",
        "npv = cm[0,0]/(cm[0,0]+cm[1,0])\n",
        "print('NPV  ', npv)\n",
        "PLR=sensitivity/(1-specificity)\n",
        "print('Positive likelihood ratio ',PLR)\n",
        "NLR=(1-sensitivity)/specificity\n",
        "print('Negative likelihood ratio ',NLR)\n",
        "DOR=PLR/NLR\n",
        "print('Diagnostic odds ratio ',DOR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion matrix \n",
            " [[36  3]\n",
            " [23  6]]\n",
            "accuray  0.6176470588235294\n",
            "sensitivity  0.20689655172413793\n",
            "specificity  0.9230769230769231\n",
            "PPV   0.6666666666666666\n",
            "NPV   0.6101694915254238\n",
            "Positive likelihood ratio  2.689655172413795\n",
            "Negative likelihood ratio  0.8591954022988506\n",
            "Diagnostic odds ratio  3.1304347826086976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtnzJfin9i3M",
        "colab_type": "code",
        "outputId": "afc536ee-7f30-469c-8a48-1cc278e00ec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "fpr, tpr, thresholds = roc_curve(test_label, np.argmax(test_prob, axis=1))\n",
        "auc_test = auc(fpr, tpr)\n",
        "auc_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-fa27dd9f4f09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mauc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mauc_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_prob' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLFsYkCJ1K4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX4ZUXE0fPg7",
        "colab_type": "code",
        "outputId": "e348cb09-309a-4a1b-8559-976b47400336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import sklearn\n",
        "cm=sklearn.metrics.confusion_matrix(train_label,train_classes)\n",
        "print('confusion matrix \\n',cm)\n",
        "total=sum(sum(cm))\n",
        "accuracy=(cm[0,0]+cm[1,1])/total\n",
        "print('accuray ',accuracy)\n",
        "sensitivity=cm[1,1]/(cm[1,1]+cm[1,0])\n",
        "print('sensitivity ',sensitivity)\n",
        "specificity=cm[0,0]/(cm[0,0]+cm[0,1])\n",
        "print('specificity ',specificity)\n",
        "ppv = cm[1,1]/(cm[1,1]+cm[0,1])\n",
        "print('PPV  ', ppv)\n",
        "npv = cm[0,0]/(cm[0,0]+cm[1,0])\n",
        "print('NPV  ', npv)\n",
        "PLR=sensitivity/(1-specificity)\n",
        "print('Positive likelihood ratio ',PLR)\n",
        "NLR=(1-sensitivity)/specificity\n",
        "print('Negative likelihood ratio ',NLR)\n",
        "DOR=PLR/NLR\n",
        "print('Diagnostic odds ratio ',DOR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion matrix \n",
            " [[2871   85]\n",
            " [ 129 1687]]\n",
            "accuray  0.9551550712489523\n",
            "sensitivity  0.9289647577092511\n",
            "specificity  0.9712449255751014\n",
            "PPV   0.9520316027088036\n",
            "NPV   0.957\n",
            "Positive likelihood ratio  32.30611557398285\n",
            "Negative likelihood ratio  0.07313834072150946\n",
            "Diagnostic odds ratio  441.71244870040994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSjniOOqbnvk",
        "colab_type": "code",
        "outputId": "1eece641-5e3c-466f-aef2-c121428349e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import sklearn\n",
        "cm=sklearn.metrics.confusion_matrix(test_label,test_classes)\n",
        "print('confusion matrix \\n',cm)\n",
        "total=sum(sum(cm))\n",
        "accuracy=(cm[0,0]+cm[1,1])/total\n",
        "print('accuray ',accuracy)\n",
        "sensitivity=cm[1,1]/(cm[1,1]+cm[1,0])\n",
        "print('sensitivity ',sensitivity)\n",
        "specificity=cm[0,0]/(cm[0,0]+cm[0,1])\n",
        "print('specificity ',specificity)\n",
        "ppv = cm[1,1]/(cm[1,1]+cm[0,1])\n",
        "print('PPV  ', ppv)\n",
        "npv = cm[0,0]/(cm[0,0]+cm[1,0])\n",
        "print('NPV  ', npv)\n",
        "PLR=sensitivity/(1-specificity)\n",
        "print('Positive likelihood ratio ',PLR)\n",
        "NLR=(1-sensitivity)/specificity\n",
        "print('Negative likelihood ratio ',NLR)\n",
        "DOR=PLR/NLR\n",
        "print('Diagnostic odds ratio ',DOR)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confusion matrix \n",
            " [[656 125]\n",
            " [166 246]]\n",
            "accuray  0.7560771165129925\n",
            "sensitivity  0.5970873786407767\n",
            "specificity  0.8399487836107554\n",
            "PPV   0.6630727762803235\n",
            "NPV   0.7980535279805353\n",
            "Positive likelihood ratio  3.730601941747572\n",
            "Negative likelihood ratio  0.47968713000236807\n",
            "Diagnostic odds ratio  7.777156626506021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dy1xLgYh7BH",
        "colab_type": "text"
      },
      "source": [
        "**combine patient slices prob**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcHL_ikMh-FY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_prediction_prob=model.predict(all_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-58KkTLkx3k1",
        "colab_type": "code",
        "outputId": "6c0b16b2-5670-4341-da19-913989ccaa3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rql2eAkxo8Kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_id_unique=np.unique(all_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOzzJr7epF5i",
        "colab_type": "code",
        "outputId": "9431ab92-a0cd-4fce-c5f5-dc2fcf3d10e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_id_unique)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "352"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCY2Q7OpkD64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds=np.argmax(all_prediction_prob,axis=1)\n",
        "prob=all_prediction_prob[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKgGbHgmj5mK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_prediction=np.array(list(zip(all_id,preds)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGyzkKpnqCu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combine_data=[]\n",
        "unique_predictions=[]\n",
        "for i in range(len(all_id_unique)):\n",
        "  ID=all_id_unique[i]\n",
        "  index=(id_prediction[:,0]==ID)\n",
        "  prediction=list(id_prediction[index][:,1])\n",
        "  if prediction.count('1')>=prediction.count('0'):\n",
        "    final_pre=1\n",
        "  else:\n",
        "    final_pre=0\n",
        "  unique_predictions.append(final_pre)\n",
        "  mix=[[ID],prediction]\n",
        "  combine_data.append(mix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzqUp57sxRow",
        "colab_type": "code",
        "outputId": "fee7a3f6-163b-4d47-e03f-1fe2b67ab782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "import sklearn\n",
        "cm=sklearn.metrics.confusion_matrix(unique_labels,unique_predictions)\n",
        "print('confusion matrix \\n',cm)\n",
        "total=sum(sum(cm))\n",
        "accuracy=(cm[0,0]+cm[1,1])/total\n",
        "print('accuray ',accuracy)\n",
        "sensitivity=cm[1,1]/(cm[1,1]+cm[1,0])\n",
        "print('sensitivity ',sensitivity)\n",
        "specificity=cm[0,0]/(cm[0,0]+cm[0,1])\n",
        "print('specificity ',specificity)\n",
        "ppv = cm[1,1]/(cm[1,1]+cm[0,1])\n",
        "print('PPV  ', ppv)\n",
        "npv = cm[0,0]/(cm[0,0]+cm[1,0])\n",
        "print('NPV  ', npv)\n",
        "PLR=sensitivity/(1-specificity)\n",
        "print('Positive likelihood ratio ',PLR)\n",
        "NLR=(1-sensitivity)/specificity\n",
        "print('Negative likelihood ratio ',NLR)\n",
        "DOR=PLR/NLR\n",
        "print('Diagnostic odds ratio ',DOR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-60c49a6d9935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munique_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'confusion matrix \\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [352, 704]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGyzxlMFzu5x",
        "colab_type": "text"
      },
      "source": [
        "**evaluate on each patient**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePd532tPzy2x",
        "colab_type": "code",
        "outputId": "ae85c650-b924-4c8a-88d7-903e47764d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i in range()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LUNG1-001'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChJo-9j7qLfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# final_table=[]\n",
        "# for i in range(len(all_id_unique)):\n",
        "#   ID=combine_data[i][0]\n",
        "#   pre=combine_data[i][1]\n",
        "#   if pre.count('1')>=pre.count('0'):\n",
        "#     final_pre=1\n",
        "#   else:\n",
        "#     final_pre=0\n",
        "#   table=[[ID],final_pre]\n",
        "#   final_table.append(table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chyp59Ktpoft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# final_decision=list([final_table[i][1] for i in range(len(all_id_unique))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo7IJjovjjId",
        "colab_type": "code",
        "outputId": "8fd0d473-01db-4258-e899-b0344f0cc54b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(all_data,to_categorical(all_label,2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7337/7337 [==============================] - 2s 333us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.7329714384036086, 0.7636636227422113]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V707zDS3JADQ",
        "colab_type": "text"
      },
      "source": [
        "Evolution of accuracy on training (blue) and validation (green) sets for 1 to 32 epochs :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsve9fEGJADQ",
        "colab_type": "text"
      },
      "source": [
        "![Accuracy evolution](pictures/scores_no_dataaugmentation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDvV6O_3JADR",
        "colab_type": "text"
      },
      "source": [
        "**After ~10 epochs the neural network reach ~70% accuracy. We can witness overfitting, no progress is made over validation set in the next epochs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZb1ZrsMJADS",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation for improving the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfRLuGz4JADS",
        "colab_type": "text"
      },
      "source": [
        "By applying random transformation to our train set, we artificially enhance our dataset with new unseen images.  \n",
        "This will hopefully reduce overfitting and allows better generalization capability for our network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1U4bAMBJADT",
        "colab_type": "text"
      },
      "source": [
        "Example of data augmentation applied to a picture:\n",
        "![Example of data augmentation applied to a picture](pictures/cat_data_augmentation.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB7y9xrUJADT",
        "colab_type": "code",
        "outputId": "b40a0ab1-14a4-4807-fe57-5128f558dcd6",
        "colab": {}
      },
      "source": [
        "train_datagen_augmented = ImageDataGenerator(\n",
        "        rescale=1./255,        # normalize pixel values to [0,1]\n",
        "        shear_range=0.2,       # randomly applies shearing transformation\n",
        "        zoom_range=0.2,        # randomly applies shearing transformation\n",
        "        horizontal_flip=True)  # randomly flip the images\n",
        "\n",
        "# same code as before\n",
        "train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 200 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CFD9FyfJADV",
        "colab_type": "code",
        "outputId": "e38b7770-68fc-4910-9a12-38a3bfa78529",
        "colab": {}
      },
      "source": [
        "##Lets look at the training data\n",
        "img = load_img('data/train/cats/cat.0.jpg')  # this is a PIL image\n",
        "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
        "x = x.reshape((1,) + x.shape)  \n",
        "\n",
        "i = 0\n",
        "for batch in train_datagen_augmented.flow(x,batch_size=1,\n",
        "                          save_to_dir='preview', save_prefix='aug2', save_format='jpg'):\n",
        "    i += 1\n",
        "    if i > 5:\n",
        "        break  #"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/train/cats/cat.0.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-028615b4661c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##Lets look at the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/train/cats/cat.0.jpg'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# this is a PIL image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# this is a Numpy array with shape (3, 150, 150)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    497\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2547\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2549\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train/cats/cat.0.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nRaBVv5JADX",
        "colab_type": "code",
        "outputId": "ef926262-1502-4aa2-b569-8021855de731",
        "colab": {}
      },
      "source": [
        "!ls preview"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aug2_0_125.jpg\t  train_10_2638.jpg  train_1_6767.jpg\ttrain_4_3297.jpg\n",
            "aug2_0_1805.jpg   train_10_3001.jpg  train_17_2432.jpg\ttrain_4_3753.jpg\n",
            "aug2_0_1983.jpg   train_11_4831.jpg  train_17_5031.jpg\ttrain_5_2054.jpg\n",
            "aug2_0_2253.jpg   train_11_6773.jpg  train_1_75.jpg\ttrain_5_6591.jpg\n",
            "aug2_0_3107.jpg   train_12_1168.jpg  train_18_8046.jpg\ttrain_6_6153.jpg\n",
            "aug2_0_4003.jpg   train_12_9061.jpg  train_18_8431.jpg\ttrain_6_8550.jpg\n",
            "aug2_0_6103.jpg   train_13_3936.jpg  train_19_6181.jpg\ttrain_7_6307.jpg\n",
            "aug2_0_6544.jpg   train_13_737.jpg   train_19_9408.jpg\ttrain_7_8120.jpg\n",
            "aug2_0_7307.jpg   train_14_3427.jpg  train_20_7450.jpg\ttrain_8_2649.jpg\n",
            "aug2_0_8106.jpg   train_14_7598.jpg  train_20_7602.jpg\ttrain_8_3558.jpg\n",
            "aug2_0_81.jpg\t  train_15_3216.jpg  train_2_1654.jpg\ttrain_9_3911.jpg\n",
            "aug2_0_8474.jpg   train_15_5589.jpg  train_2_8433.jpg\ttrain_9_5143.jpg\n",
            "train_0_5372.jpg  train_16_172.jpg   train_3_2508.jpg\n",
            "train_0_7289.jpg  train_16_3743.jpg  train_3_3853.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF6yxA8iJADl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.load_weights('models_trained/augmented_30_epochs.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaDh9Z_DJADm",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating on validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVUKbtyBJADn",
        "colab_type": "text"
      },
      "source": [
        "Computing loss and accuracy :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TruG7XwsJADn",
        "colab_type": "code",
        "outputId": "b02e8b13-2669-40a0-8778-041221bdc62d",
        "colab": {}
      },
      "source": [
        "model.evaluate_generator(validation_generator, validation_samples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.57709803022086048, 0.76551231971153844]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwUd-sWEJADp",
        "colab_type": "text"
      },
      "source": [
        "Evolution of accuracy on training (blue) and validation (green) sets for 1 to 100 epochs :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3vVeZFNJADq",
        "colab_type": "text"
      },
      "source": [
        "![Accuracy evolution](pictures/scores_with_dataaugmentation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cPHgftcJADq",
        "colab_type": "text"
      },
      "source": [
        "**Thanks to data-augmentation, the accuracy on the validation set improved to ~80%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zzy9R7vJADr",
        "colab_type": "text"
      },
      "source": [
        "## Using a pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8PG66p3JADs",
        "colab_type": "text"
      },
      "source": [
        "The process of training a convolutionnal neural network can be very time-consuming and require a lot of datas.  \n",
        "\n",
        "We can go beyond the previous models in terms of performance and efficiency by using a general-purpose, pre-trained image classifier.  This example uses VGG16, a model trained on the ImageNet dataset - which contains millions of images classified in 1000 categories. \n",
        "\n",
        "On top of it, we add a small multi-layer perceptron and we train it on our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FeyzVdBJADt",
        "colab_type": "text"
      },
      "source": [
        "### VGG16 + small MLP\n",
        "![VGG16 + Dense layers Schema](pictures/vgg16_original.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJpnwjU8JADt",
        "colab_type": "text"
      },
      "source": [
        "#### VGG16 model is available in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIeVwEB_KzQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import ResNet50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBx1Zx8TJADu",
        "colab_type": "code",
        "outputId": "7e41f84f-1d77-4a6c-8fd4-b6db0c8ee6b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "model_pretrain = applications.resnet.ResNet50(input_shape=(128,128,3),include_top=False, weights='imagenet')\n",
        "epochs = 300\n",
        "train_samples = len(train_id)\n",
        "validation_samples = len(inval_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-72bad468e1e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_pretrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalidation_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minval_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras.applications' has no attribute 'resnet'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqewqw74OBBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_QdBtffCc1P",
        "colab_type": "code",
        "outputId": "e9b3ab79-dfc7-49af-e3ca-6a14ac41c035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "batch_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6c43fa1057c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvlCN0nsJADw",
        "colab_type": "text"
      },
      "source": [
        "### Using the VGG16 model to process samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w-CNJvWJADx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_3chanel=np.squeeze(np.stack((train_data,) * 3, -1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsuLdWdCHs8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_3chanel=np.squeeze(np.stack((inval_data,) * 3, -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8QiEaFqJAD1",
        "colab_type": "code",
        "outputId": "63c6eef1-6ec5-48d6-c13e-abfd8e618a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "model_pretrain.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl7pP0LkPCrS",
        "colab_type": "code",
        "outputId": "b8bbb43d-9de8-4a40-b459-6cf24b4a61e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 11.8 GB  | Proc size: 120.0 MB\n",
            "GPU RAM Free: 8938MB | Used: 2503MB | Util  22% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKAYqlyNJAD8",
        "colab_type": "text"
      },
      "source": [
        "This is a long process, so we save the output of the VGG16 once and for all.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX6Y-6zb6Qs1",
        "colab_type": "code",
        "outputId": "8c2c027e-fd99-4be9-daee-8cb78e63a0c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "bottleneck_features_train = model_pretrain.predict(train_3chanel,verbose=1)\n",
        "print('done for train')\n",
        "np.save(open('/content/drive/My Drive/gpu/bottleneck_features_train.npy', 'wb'), bottleneck_features_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4809/4809 [==============================] - 8s 2ms/step\n",
            "done for train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPxw2CdY_38P",
        "colab_type": "code",
        "outputId": "78201df3-921e-4882-f08e-770ac62e8b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bottleneck_features_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4809, 4, 4, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZhRX6aL6mOi",
        "colab_type": "code",
        "outputId": "eec09225-6b0a-454d-89b1-d02367a00ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bottleneck_features_train1.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2809, 4, 4, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xarfoz-qK804",
        "colab_type": "code",
        "outputId": "fe48495d-8a3c-433e-a570-512c9685f9de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "bottleneck_features_validation = model_pretrain.predict(test_3chanel,verbose=1)\n",
        "print('done for test')\n",
        "np.save(open('/content/drive/My Drive/gpu/bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1346/1346 [==============================] - 3s 2ms/step\n",
            "done for test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nhh89ZAcJAD_",
        "colab_type": "text"
      },
      "source": [
        "Now we can load it..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuYkTu2nJAEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_feature = np.load(open('/content/drive/My Drive/gpu/bottleneck_features_train.npy', 'rb'))\n",
        "train_labels = train_label\n",
        "\n",
        "validation_feature = np.load(open('/content/drive/My Drive/gpu/bottleneck_features_validation.npy', 'rb'))\n",
        "validation_labels = inval_label\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eImc2g7K7IFA",
        "colab_type": "code",
        "outputId": "a69143f9-bc24-4494-9039-1ed0b0ec331b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2809, 4, 4, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPtzo7WIJAEE",
        "colab_type": "text"
      },
      "source": [
        "And define and train the custom fully connected neural network :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIo83ViTJAEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_top = Sequential()\n",
        "model_top.add(Flatten(input_shape=train_feature.shape[1:]))\n",
        "model_top.add(Dense(512, activation='relu'))\n",
        "model_top.add(Dropout(0.5))\n",
        "model_top.add(Dense(256, activation='relu'))\n",
        "model_top.add(Dropout(0.5))\n",
        "model_top.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_top.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oEzUeR8Al-L",
        "colab_type": "code",
        "outputId": "7b95efbb-a17c-4e28-b582-88e518f0c831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "model_top.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_6 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 512)               4194816   \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 4,326,401\n",
            "Trainable params: 4,326,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jnm-PY0EIDQ4",
        "colab_type": "code",
        "outputId": "b4299fc0-5cce-4e61-aa97-bb64197b7495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "inval_label.count(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-eaa69546ef3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minval_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'count'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpnUnCCNJAEH",
        "colab_type": "code",
        "outputId": "3d87267e-ad44-42e0-c018-d711365f16bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10254
        }
      },
      "source": [
        "top_history=model_top.fit(train_feature, train_labels,\n",
        "        epochs=epochs, \n",
        "        batch_size=batch_size,\n",
        "        validation_data=(validation_feature, validation_labels))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4809 samples, validate on 1346 samples\n",
            "Epoch 1/300\n",
            "4809/4809 [==============================] - 1s 250us/step - loss: 0.8788 - acc: 0.6082 - val_loss: 0.6506 - val_acc: 0.6538\n",
            "Epoch 2/300\n",
            "4809/4809 [==============================] - 1s 143us/step - loss: 0.6447 - acc: 0.6475 - val_loss: 0.7284 - val_acc: 0.6538\n",
            "Epoch 3/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.6173 - acc: 0.6611 - val_loss: 0.6410 - val_acc: 0.6501\n",
            "Epoch 4/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.5918 - acc: 0.6619 - val_loss: 0.8861 - val_acc: 0.6538\n",
            "Epoch 5/300\n",
            "4809/4809 [==============================] - 1s 139us/step - loss: 0.5752 - acc: 0.6764 - val_loss: 0.6818 - val_acc: 0.6152\n",
            "Epoch 6/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.5632 - acc: 0.6812 - val_loss: 0.7216 - val_acc: 0.6166\n",
            "Epoch 7/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.5551 - acc: 0.6873 - val_loss: 0.7729 - val_acc: 0.6575\n",
            "Epoch 8/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.5385 - acc: 0.6931 - val_loss: 0.8745 - val_acc: 0.6456\n",
            "Epoch 9/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.5236 - acc: 0.6995 - val_loss: 1.1042 - val_acc: 0.6634\n",
            "Epoch 10/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.5139 - acc: 0.7209 - val_loss: 0.9904 - val_acc: 0.6597\n",
            "Epoch 11/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.4984 - acc: 0.7193 - val_loss: 0.9024 - val_acc: 0.5758\n",
            "Epoch 12/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.4952 - acc: 0.7203 - val_loss: 1.2379 - val_acc: 0.6523\n",
            "Epoch 13/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.4878 - acc: 0.7315 - val_loss: 0.9112 - val_acc: 0.5572\n",
            "Epoch 14/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.4796 - acc: 0.7361 - val_loss: 1.5402 - val_acc: 0.6530\n",
            "Epoch 15/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.4680 - acc: 0.7446 - val_loss: 1.3263 - val_acc: 0.6612\n",
            "Epoch 16/300\n",
            "4809/4809 [==============================] - 1s 140us/step - loss: 0.4580 - acc: 0.7430 - val_loss: 0.9942 - val_acc: 0.5654\n",
            "Epoch 17/300\n",
            "4809/4809 [==============================] - 1s 146us/step - loss: 0.4494 - acc: 0.7598 - val_loss: 1.1703 - val_acc: 0.6233\n",
            "Epoch 18/300\n",
            "4809/4809 [==============================] - 1s 144us/step - loss: 0.4475 - acc: 0.7588 - val_loss: 1.0877 - val_acc: 0.6419\n",
            "Epoch 19/300\n",
            "4809/4809 [==============================] - 1s 148us/step - loss: 0.4506 - acc: 0.7546 - val_loss: 1.1779 - val_acc: 0.6404\n",
            "Epoch 20/300\n",
            "4809/4809 [==============================] - 1s 146us/step - loss: 0.4242 - acc: 0.7773 - val_loss: 1.2528 - val_acc: 0.5765\n",
            "Epoch 21/300\n",
            "4809/4809 [==============================] - 1s 144us/step - loss: 0.4310 - acc: 0.7661 - val_loss: 1.6550 - val_acc: 0.6634\n",
            "Epoch 22/300\n",
            "4809/4809 [==============================] - 1s 150us/step - loss: 0.4256 - acc: 0.7796 - val_loss: 1.3171 - val_acc: 0.5468\n",
            "Epoch 23/300\n",
            "4809/4809 [==============================] - 1s 149us/step - loss: 0.4243 - acc: 0.7790 - val_loss: 1.3877 - val_acc: 0.6352\n",
            "Epoch 24/300\n",
            "4809/4809 [==============================] - 1s 147us/step - loss: 0.4172 - acc: 0.7877 - val_loss: 1.6198 - val_acc: 0.6456\n",
            "Epoch 25/300\n",
            "4809/4809 [==============================] - 1s 146us/step - loss: 0.3957 - acc: 0.7948 - val_loss: 1.7501 - val_acc: 0.6523\n",
            "Epoch 26/300\n",
            "4809/4809 [==============================] - 1s 150us/step - loss: 0.4059 - acc: 0.7829 - val_loss: 1.9910 - val_acc: 0.6605\n",
            "Epoch 27/300\n",
            "4809/4809 [==============================] - 1s 148us/step - loss: 0.4006 - acc: 0.7844 - val_loss: 1.6564 - val_acc: 0.6159\n",
            "Epoch 28/300\n",
            "4809/4809 [==============================] - 1s 145us/step - loss: 0.3943 - acc: 0.8020 - val_loss: 1.1899 - val_acc: 0.5899\n",
            "Epoch 29/300\n",
            "4809/4809 [==============================] - 1s 148us/step - loss: 0.3773 - acc: 0.8010 - val_loss: 1.2673 - val_acc: 0.5862\n",
            "Epoch 30/300\n",
            "4809/4809 [==============================] - 1s 142us/step - loss: 0.3813 - acc: 0.8070 - val_loss: 1.3221 - val_acc: 0.5869\n",
            "Epoch 31/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.3813 - acc: 0.8018 - val_loss: 1.6026 - val_acc: 0.6352\n",
            "Epoch 32/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.3795 - acc: 0.8104 - val_loss: 1.7374 - val_acc: 0.6404\n",
            "Epoch 33/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.3715 - acc: 0.8122 - val_loss: 1.4532 - val_acc: 0.6256\n",
            "Epoch 34/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.3687 - acc: 0.8089 - val_loss: 1.3733 - val_acc: 0.5446\n",
            "Epoch 35/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.3643 - acc: 0.8116 - val_loss: 1.5866 - val_acc: 0.6100\n",
            "Epoch 36/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.3625 - acc: 0.8203 - val_loss: 1.5615 - val_acc: 0.5557\n",
            "Epoch 37/300\n",
            "4809/4809 [==============================] - 1s 138us/step - loss: 0.3647 - acc: 0.8153 - val_loss: 1.7431 - val_acc: 0.6033\n",
            "Epoch 38/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.3469 - acc: 0.8228 - val_loss: 1.7968 - val_acc: 0.6367\n",
            "Epoch 39/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.3544 - acc: 0.8239 - val_loss: 1.7740 - val_acc: 0.6122\n",
            "Epoch 40/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.3565 - acc: 0.8193 - val_loss: 1.5989 - val_acc: 0.6085\n",
            "Epoch 41/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.3460 - acc: 0.8205 - val_loss: 1.9260 - val_acc: 0.5840\n",
            "Epoch 42/300\n",
            "4809/4809 [==============================] - 1s 132us/step - loss: 0.3437 - acc: 0.8249 - val_loss: 1.6093 - val_acc: 0.5773\n",
            "Epoch 43/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.3349 - acc: 0.8322 - val_loss: 1.9058 - val_acc: 0.6122\n",
            "Epoch 44/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.3241 - acc: 0.8372 - val_loss: 1.9131 - val_acc: 0.6189\n",
            "Epoch 45/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.3400 - acc: 0.8264 - val_loss: 1.7705 - val_acc: 0.5929\n",
            "Epoch 46/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.3400 - acc: 0.8239 - val_loss: 1.8694 - val_acc: 0.5921\n",
            "Epoch 47/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.3312 - acc: 0.8334 - val_loss: 1.8476 - val_acc: 0.6144\n",
            "Epoch 48/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.3310 - acc: 0.8332 - val_loss: 2.0224 - val_acc: 0.6508\n",
            "Epoch 49/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.3321 - acc: 0.8378 - val_loss: 1.9272 - val_acc: 0.6345\n",
            "Epoch 50/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.3266 - acc: 0.8409 - val_loss: 1.6271 - val_acc: 0.6055\n",
            "Epoch 51/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.3192 - acc: 0.8391 - val_loss: 1.6333 - val_acc: 0.5587\n",
            "Epoch 52/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.3226 - acc: 0.8374 - val_loss: 2.5092 - val_acc: 0.6471\n",
            "Epoch 53/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.3199 - acc: 0.8422 - val_loss: 1.9460 - val_acc: 0.5996\n",
            "Epoch 54/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.3303 - acc: 0.8428 - val_loss: 1.6904 - val_acc: 0.5847\n",
            "Epoch 55/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.3279 - acc: 0.8418 - val_loss: 2.0471 - val_acc: 0.5921\n",
            "Epoch 56/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.3102 - acc: 0.8515 - val_loss: 2.0769 - val_acc: 0.6426\n",
            "Epoch 57/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.3001 - acc: 0.8517 - val_loss: 2.0280 - val_acc: 0.5884\n",
            "Epoch 58/300\n",
            "4809/4809 [==============================] - 1s 148us/step - loss: 0.3112 - acc: 0.8494 - val_loss: 1.8885 - val_acc: 0.5453\n",
            "Epoch 59/300\n",
            "4809/4809 [==============================] - 1s 138us/step - loss: 0.3062 - acc: 0.8486 - val_loss: 1.7688 - val_acc: 0.5676\n",
            "Epoch 60/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.3152 - acc: 0.8457 - val_loss: 2.2276 - val_acc: 0.6025\n",
            "Epoch 61/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.3009 - acc: 0.8486 - val_loss: 2.2884 - val_acc: 0.6322\n",
            "Epoch 62/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.3001 - acc: 0.8551 - val_loss: 2.1873 - val_acc: 0.6122\n",
            "Epoch 63/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2946 - acc: 0.8546 - val_loss: 2.6583 - val_acc: 0.6568\n",
            "Epoch 64/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.3058 - acc: 0.8563 - val_loss: 2.1035 - val_acc: 0.6204\n",
            "Epoch 65/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2915 - acc: 0.8613 - val_loss: 2.5522 - val_acc: 0.6360\n",
            "Epoch 66/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2843 - acc: 0.8665 - val_loss: 2.0244 - val_acc: 0.5840\n",
            "Epoch 67/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.3021 - acc: 0.8549 - val_loss: 2.2006 - val_acc: 0.6241\n",
            "Epoch 68/300\n",
            "4809/4809 [==============================] - 1s 138us/step - loss: 0.2932 - acc: 0.8594 - val_loss: 2.1875 - val_acc: 0.6085\n",
            "Epoch 69/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.3020 - acc: 0.8559 - val_loss: 2.0082 - val_acc: 0.5810\n",
            "Epoch 70/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2955 - acc: 0.8574 - val_loss: 2.0926 - val_acc: 0.6189\n",
            "Epoch 71/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2962 - acc: 0.8511 - val_loss: 2.4445 - val_acc: 0.6248\n",
            "Epoch 72/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2916 - acc: 0.8592 - val_loss: 2.1473 - val_acc: 0.6062\n",
            "Epoch 73/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2913 - acc: 0.8655 - val_loss: 2.5791 - val_acc: 0.6256\n",
            "Epoch 74/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2991 - acc: 0.8532 - val_loss: 2.4362 - val_acc: 0.6152\n",
            "Epoch 75/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2852 - acc: 0.8661 - val_loss: 2.2331 - val_acc: 0.6315\n",
            "Epoch 76/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2859 - acc: 0.8621 - val_loss: 2.3506 - val_acc: 0.5936\n",
            "Epoch 77/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2834 - acc: 0.8661 - val_loss: 2.6254 - val_acc: 0.6516\n",
            "Epoch 78/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2805 - acc: 0.8644 - val_loss: 2.3327 - val_acc: 0.6218\n",
            "Epoch 79/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2864 - acc: 0.8644 - val_loss: 2.2094 - val_acc: 0.5988\n",
            "Epoch 80/300\n",
            "4809/4809 [==============================] - 1s 132us/step - loss: 0.2846 - acc: 0.8688 - val_loss: 2.3861 - val_acc: 0.6263\n",
            "Epoch 81/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2902 - acc: 0.8644 - val_loss: 2.5162 - val_acc: 0.6278\n",
            "Epoch 82/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2718 - acc: 0.8640 - val_loss: 2.3045 - val_acc: 0.6122\n",
            "Epoch 83/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2759 - acc: 0.8642 - val_loss: 2.2995 - val_acc: 0.6263\n",
            "Epoch 84/300\n",
            "4809/4809 [==============================] - 1s 138us/step - loss: 0.2762 - acc: 0.8734 - val_loss: 2.4783 - val_acc: 0.6189\n",
            "Epoch 85/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2804 - acc: 0.8684 - val_loss: 2.5575 - val_acc: 0.6107\n",
            "Epoch 86/300\n",
            "4809/4809 [==============================] - 1s 132us/step - loss: 0.2649 - acc: 0.8732 - val_loss: 2.4422 - val_acc: 0.6114\n",
            "Epoch 87/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2742 - acc: 0.8725 - val_loss: 3.1311 - val_acc: 0.6426\n",
            "Epoch 88/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2889 - acc: 0.8590 - val_loss: 2.4423 - val_acc: 0.6129\n",
            "Epoch 89/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2735 - acc: 0.8711 - val_loss: 2.4946 - val_acc: 0.6025\n",
            "Epoch 90/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2812 - acc: 0.8709 - val_loss: 2.4574 - val_acc: 0.5899\n",
            "Epoch 91/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2746 - acc: 0.8682 - val_loss: 1.9927 - val_acc: 0.5750\n",
            "Epoch 92/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2746 - acc: 0.8686 - val_loss: 2.3848 - val_acc: 0.6003\n",
            "Epoch 93/300\n",
            "4809/4809 [==============================] - 1s 138us/step - loss: 0.2726 - acc: 0.8750 - val_loss: 2.5475 - val_acc: 0.6159\n",
            "Epoch 94/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2746 - acc: 0.8705 - val_loss: 2.1346 - val_acc: 0.5944\n",
            "Epoch 95/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2646 - acc: 0.8742 - val_loss: 2.3209 - val_acc: 0.6122\n",
            "Epoch 96/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2751 - acc: 0.8736 - val_loss: 2.6604 - val_acc: 0.6218\n",
            "Epoch 97/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2658 - acc: 0.8752 - val_loss: 2.5299 - val_acc: 0.5981\n",
            "Epoch 98/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2667 - acc: 0.8804 - val_loss: 2.7869 - val_acc: 0.6456\n",
            "Epoch 99/300\n",
            "4809/4809 [==============================] - 1s 138us/step - loss: 0.2726 - acc: 0.8754 - val_loss: 2.5287 - val_acc: 0.6107\n",
            "Epoch 100/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2694 - acc: 0.8740 - val_loss: 2.3549 - val_acc: 0.5810\n",
            "Epoch 101/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2804 - acc: 0.8734 - val_loss: 2.3690 - val_acc: 0.6040\n",
            "Epoch 102/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2626 - acc: 0.8779 - val_loss: 2.4356 - val_acc: 0.6226\n",
            "Epoch 103/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2731 - acc: 0.8686 - val_loss: 2.1972 - val_acc: 0.6122\n",
            "Epoch 104/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2615 - acc: 0.8773 - val_loss: 2.9288 - val_acc: 0.6285\n",
            "Epoch 105/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2561 - acc: 0.8796 - val_loss: 2.4661 - val_acc: 0.5869\n",
            "Epoch 106/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.2603 - acc: 0.8800 - val_loss: 2.5810 - val_acc: 0.5936\n",
            "Epoch 107/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2576 - acc: 0.8819 - val_loss: 2.3748 - val_acc: 0.5884\n",
            "Epoch 108/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2618 - acc: 0.8763 - val_loss: 3.6931 - val_acc: 0.6449\n",
            "Epoch 109/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2570 - acc: 0.8802 - val_loss: 2.5106 - val_acc: 0.5906\n",
            "Epoch 110/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2506 - acc: 0.8858 - val_loss: 3.0464 - val_acc: 0.6308\n",
            "Epoch 111/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2507 - acc: 0.8856 - val_loss: 2.5809 - val_acc: 0.5847\n",
            "Epoch 112/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2589 - acc: 0.8831 - val_loss: 2.8911 - val_acc: 0.6300\n",
            "Epoch 113/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.2508 - acc: 0.8875 - val_loss: 2.7732 - val_acc: 0.6389\n",
            "Epoch 114/300\n",
            "4809/4809 [==============================] - 1s 149us/step - loss: 0.2610 - acc: 0.8840 - val_loss: 2.5634 - val_acc: 0.5899\n",
            "Epoch 115/300\n",
            "4809/4809 [==============================] - 1s 151us/step - loss: 0.2490 - acc: 0.8844 - val_loss: 2.2814 - val_acc: 0.5810\n",
            "Epoch 116/300\n",
            "4809/4809 [==============================] - 1s 149us/step - loss: 0.2664 - acc: 0.8811 - val_loss: 2.8262 - val_acc: 0.6330\n",
            "Epoch 117/300\n",
            "4809/4809 [==============================] - 1s 147us/step - loss: 0.2554 - acc: 0.8846 - val_loss: 2.4519 - val_acc: 0.5802\n",
            "Epoch 118/300\n",
            "4809/4809 [==============================] - 1s 151us/step - loss: 0.2533 - acc: 0.8798 - val_loss: 2.5551 - val_acc: 0.6025\n",
            "Epoch 119/300\n",
            "4809/4809 [==============================] - 1s 150us/step - loss: 0.2494 - acc: 0.8879 - val_loss: 2.4900 - val_acc: 0.6033\n",
            "Epoch 120/300\n",
            "4809/4809 [==============================] - 1s 149us/step - loss: 0.2486 - acc: 0.8888 - val_loss: 2.4939 - val_acc: 0.5802\n",
            "Epoch 121/300\n",
            "4809/4809 [==============================] - 1s 138us/step - loss: 0.2558 - acc: 0.8823 - val_loss: 2.7781 - val_acc: 0.6293\n",
            "Epoch 122/300\n",
            "4809/4809 [==============================] - 1s 139us/step - loss: 0.2580 - acc: 0.8798 - val_loss: 2.4554 - val_acc: 0.6174\n",
            "Epoch 123/300\n",
            "4809/4809 [==============================] - 1s 132us/step - loss: 0.2503 - acc: 0.8840 - val_loss: 2.8581 - val_acc: 0.6285\n",
            "Epoch 124/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2530 - acc: 0.8817 - val_loss: 2.5691 - val_acc: 0.5988\n",
            "Epoch 125/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2453 - acc: 0.8912 - val_loss: 2.3961 - val_acc: 0.5921\n",
            "Epoch 126/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2561 - acc: 0.8825 - val_loss: 2.9993 - val_acc: 0.6464\n",
            "Epoch 127/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2585 - acc: 0.8858 - val_loss: 2.7794 - val_acc: 0.6196\n",
            "Epoch 128/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2473 - acc: 0.8888 - val_loss: 2.3563 - val_acc: 0.5765\n",
            "Epoch 129/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2582 - acc: 0.8819 - val_loss: 2.7070 - val_acc: 0.6263\n",
            "Epoch 130/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2460 - acc: 0.8881 - val_loss: 3.0150 - val_acc: 0.6263\n",
            "Epoch 131/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2503 - acc: 0.8885 - val_loss: 2.6560 - val_acc: 0.6114\n",
            "Epoch 132/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2442 - acc: 0.8917 - val_loss: 3.1040 - val_acc: 0.6189\n",
            "Epoch 133/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2563 - acc: 0.8858 - val_loss: 2.4421 - val_acc: 0.5847\n",
            "Epoch 134/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2337 - acc: 0.8969 - val_loss: 2.9642 - val_acc: 0.6092\n",
            "Epoch 135/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2409 - acc: 0.8900 - val_loss: 2.7691 - val_acc: 0.6010\n",
            "Epoch 136/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2467 - acc: 0.8919 - val_loss: 2.9349 - val_acc: 0.6308\n",
            "Epoch 137/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2364 - acc: 0.8894 - val_loss: 3.5422 - val_acc: 0.6456\n",
            "Epoch 138/300\n",
            "4809/4809 [==============================] - 1s 144us/step - loss: 0.2516 - acc: 0.8873 - val_loss: 2.5271 - val_acc: 0.5973\n",
            "Epoch 139/300\n",
            "4809/4809 [==============================] - 1s 147us/step - loss: 0.2303 - acc: 0.8964 - val_loss: 2.7691 - val_acc: 0.5899\n",
            "Epoch 140/300\n",
            "4809/4809 [==============================] - 1s 147us/step - loss: 0.2441 - acc: 0.8894 - val_loss: 3.0504 - val_acc: 0.6166\n",
            "Epoch 141/300\n",
            "4809/4809 [==============================] - 1s 148us/step - loss: 0.2468 - acc: 0.8908 - val_loss: 2.6386 - val_acc: 0.5884\n",
            "Epoch 142/300\n",
            "4809/4809 [==============================] - 1s 148us/step - loss: 0.2435 - acc: 0.8873 - val_loss: 2.6652 - val_acc: 0.6152\n",
            "Epoch 143/300\n",
            "4809/4809 [==============================] - 1s 145us/step - loss: 0.2522 - acc: 0.8873 - val_loss: 2.7983 - val_acc: 0.6018\n",
            "Epoch 144/300\n",
            "4809/4809 [==============================] - 1s 148us/step - loss: 0.2411 - acc: 0.8863 - val_loss: 2.7706 - val_acc: 0.5832\n",
            "Epoch 145/300\n",
            "4809/4809 [==============================] - 1s 147us/step - loss: 0.2648 - acc: 0.8815 - val_loss: 2.7228 - val_acc: 0.5944\n",
            "Epoch 146/300\n",
            "4809/4809 [==============================] - 1s 148us/step - loss: 0.2330 - acc: 0.8962 - val_loss: 3.2207 - val_acc: 0.6218\n",
            "Epoch 147/300\n",
            "4809/4809 [==============================] - 1s 147us/step - loss: 0.2530 - acc: 0.8873 - val_loss: 2.6998 - val_acc: 0.5906\n",
            "Epoch 148/300\n",
            "4809/4809 [==============================] - 1s 150us/step - loss: 0.2551 - acc: 0.8900 - val_loss: 2.4081 - val_acc: 0.5817\n",
            "Epoch 149/300\n",
            "4809/4809 [==============================] - 1s 159us/step - loss: 0.2526 - acc: 0.8875 - val_loss: 2.5932 - val_acc: 0.5609\n",
            "Epoch 150/300\n",
            "4809/4809 [==============================] - 1s 151us/step - loss: 0.2440 - acc: 0.8969 - val_loss: 2.6997 - val_acc: 0.5988\n",
            "Epoch 151/300\n",
            "4809/4809 [==============================] - 1s 147us/step - loss: 0.2447 - acc: 0.8863 - val_loss: 2.8046 - val_acc: 0.6100\n",
            "Epoch 152/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2481 - acc: 0.8921 - val_loss: 2.4211 - val_acc: 0.5825\n",
            "Epoch 153/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2350 - acc: 0.8931 - val_loss: 3.0415 - val_acc: 0.6196\n",
            "Epoch 154/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2497 - acc: 0.8879 - val_loss: 2.4906 - val_acc: 0.6114\n",
            "Epoch 155/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2470 - acc: 0.8950 - val_loss: 2.8828 - val_acc: 0.6218\n",
            "Epoch 156/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2298 - acc: 0.9043 - val_loss: 2.6275 - val_acc: 0.5698\n",
            "Epoch 157/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2349 - acc: 0.8890 - val_loss: 2.8606 - val_acc: 0.5944\n",
            "Epoch 158/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.2359 - acc: 0.8969 - val_loss: 2.8936 - val_acc: 0.6077\n",
            "Epoch 159/300\n",
            "4809/4809 [==============================] - 1s 154us/step - loss: 0.2331 - acc: 0.9006 - val_loss: 3.2228 - val_acc: 0.6337\n",
            "Epoch 160/300\n",
            "4809/4809 [==============================] - 1s 151us/step - loss: 0.2483 - acc: 0.8917 - val_loss: 3.0566 - val_acc: 0.6233\n",
            "Epoch 161/300\n",
            "4809/4809 [==============================] - 1s 153us/step - loss: 0.2330 - acc: 0.8956 - val_loss: 3.0347 - val_acc: 0.6114\n",
            "Epoch 162/300\n",
            "4809/4809 [==============================] - 1s 154us/step - loss: 0.2338 - acc: 0.8933 - val_loss: 3.2613 - val_acc: 0.6285\n",
            "Epoch 163/300\n",
            "4809/4809 [==============================] - 1s 153us/step - loss: 0.2327 - acc: 0.8994 - val_loss: 2.8113 - val_acc: 0.6018\n",
            "Epoch 164/300\n",
            "4809/4809 [==============================] - 1s 153us/step - loss: 0.2346 - acc: 0.8979 - val_loss: 2.5270 - val_acc: 0.5914\n",
            "Epoch 165/300\n",
            "4809/4809 [==============================] - 1s 150us/step - loss: 0.2417 - acc: 0.8962 - val_loss: 2.9810 - val_acc: 0.6062\n",
            "Epoch 166/300\n",
            "4809/4809 [==============================] - 1s 152us/step - loss: 0.2371 - acc: 0.8975 - val_loss: 2.7766 - val_acc: 0.5921\n",
            "Epoch 167/300\n",
            "4809/4809 [==============================] - 1s 151us/step - loss: 0.2381 - acc: 0.8921 - val_loss: 3.0067 - val_acc: 0.6218\n",
            "Epoch 168/300\n",
            "4809/4809 [==============================] - 1s 149us/step - loss: 0.2339 - acc: 0.8962 - val_loss: 2.8949 - val_acc: 0.6070\n",
            "Epoch 169/300\n",
            "4809/4809 [==============================] - 1s 152us/step - loss: 0.2472 - acc: 0.8896 - val_loss: 2.8816 - val_acc: 0.5966\n",
            "Epoch 170/300\n",
            "4809/4809 [==============================] - 1s 153us/step - loss: 0.2352 - acc: 0.8967 - val_loss: 3.2737 - val_acc: 0.6204\n",
            "Epoch 171/300\n",
            "4809/4809 [==============================] - 1s 152us/step - loss: 0.2399 - acc: 0.8923 - val_loss: 2.9256 - val_acc: 0.6003\n",
            "Epoch 172/300\n",
            "4809/4809 [==============================] - 1s 146us/step - loss: 0.2267 - acc: 0.9002 - val_loss: 2.9450 - val_acc: 0.5973\n",
            "Epoch 173/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2284 - acc: 0.8991 - val_loss: 2.9126 - val_acc: 0.6018\n",
            "Epoch 174/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2443 - acc: 0.8925 - val_loss: 2.8708 - val_acc: 0.5951\n",
            "Epoch 175/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2347 - acc: 0.8975 - val_loss: 2.6199 - val_acc: 0.5862\n",
            "Epoch 176/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2427 - acc: 0.8994 - val_loss: 2.6856 - val_acc: 0.5973\n",
            "Epoch 177/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2379 - acc: 0.8985 - val_loss: 3.0540 - val_acc: 0.6114\n",
            "Epoch 178/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2462 - acc: 0.8902 - val_loss: 3.1986 - val_acc: 0.6270\n",
            "Epoch 179/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.2348 - acc: 0.8956 - val_loss: 2.9472 - val_acc: 0.6129\n",
            "Epoch 180/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2425 - acc: 0.8898 - val_loss: 2.8871 - val_acc: 0.6055\n",
            "Epoch 181/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2363 - acc: 0.8956 - val_loss: 2.7857 - val_acc: 0.5892\n",
            "Epoch 182/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2320 - acc: 0.8991 - val_loss: 3.4368 - val_acc: 0.6300\n",
            "Epoch 183/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2341 - acc: 0.8952 - val_loss: 3.0255 - val_acc: 0.6174\n",
            "Epoch 184/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2508 - acc: 0.8904 - val_loss: 2.8870 - val_acc: 0.5996\n",
            "Epoch 185/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2344 - acc: 0.8948 - val_loss: 3.2442 - val_acc: 0.6092\n",
            "Epoch 186/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2331 - acc: 0.8979 - val_loss: 2.9128 - val_acc: 0.6114\n",
            "Epoch 187/300\n",
            "4809/4809 [==============================] - 1s 138us/step - loss: 0.2462 - acc: 0.8946 - val_loss: 2.9999 - val_acc: 0.5988\n",
            "Epoch 188/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2302 - acc: 0.8985 - val_loss: 2.6931 - val_acc: 0.5788\n",
            "Epoch 189/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2319 - acc: 0.8989 - val_loss: 2.8206 - val_acc: 0.5996\n",
            "Epoch 190/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2431 - acc: 0.8904 - val_loss: 2.9112 - val_acc: 0.6070\n",
            "Epoch 191/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2426 - acc: 0.8888 - val_loss: 2.8164 - val_acc: 0.6077\n",
            "Epoch 192/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2327 - acc: 0.9016 - val_loss: 2.8879 - val_acc: 0.6048\n",
            "Epoch 193/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2341 - acc: 0.8952 - val_loss: 2.8360 - val_acc: 0.6010\n",
            "Epoch 194/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2317 - acc: 0.8985 - val_loss: 2.8989 - val_acc: 0.5825\n",
            "Epoch 195/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2257 - acc: 0.9002 - val_loss: 3.0373 - val_acc: 0.6174\n",
            "Epoch 196/300\n",
            "4809/4809 [==============================] - 1s 132us/step - loss: 0.2345 - acc: 0.8971 - val_loss: 2.9694 - val_acc: 0.6033\n",
            "Epoch 197/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2402 - acc: 0.8921 - val_loss: 2.8343 - val_acc: 0.6055\n",
            "Epoch 198/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2369 - acc: 0.8937 - val_loss: 2.8323 - val_acc: 0.6107\n",
            "Epoch 199/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2327 - acc: 0.8906 - val_loss: 2.9338 - val_acc: 0.5966\n",
            "Epoch 200/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2518 - acc: 0.8977 - val_loss: 2.7789 - val_acc: 0.5966\n",
            "Epoch 201/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2410 - acc: 0.8946 - val_loss: 2.8833 - val_acc: 0.5921\n",
            "Epoch 202/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2335 - acc: 0.8952 - val_loss: 2.9176 - val_acc: 0.5958\n",
            "Epoch 203/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2349 - acc: 0.9031 - val_loss: 2.9617 - val_acc: 0.5899\n",
            "Epoch 204/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2344 - acc: 0.9010 - val_loss: 2.6943 - val_acc: 0.5765\n",
            "Epoch 205/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2407 - acc: 0.8948 - val_loss: 2.7942 - val_acc: 0.5832\n",
            "Epoch 206/300\n",
            "4809/4809 [==============================] - 1s 138us/step - loss: 0.2345 - acc: 0.8979 - val_loss: 3.4934 - val_acc: 0.6278\n",
            "Epoch 207/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2378 - acc: 0.8931 - val_loss: 2.8492 - val_acc: 0.5840\n",
            "Epoch 208/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2166 - acc: 0.9041 - val_loss: 3.1431 - val_acc: 0.6003\n",
            "Epoch 209/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2332 - acc: 0.8960 - val_loss: 3.2882 - val_acc: 0.6129\n",
            "Epoch 210/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2349 - acc: 0.8981 - val_loss: 2.7917 - val_acc: 0.5884\n",
            "Epoch 211/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2258 - acc: 0.9041 - val_loss: 2.9202 - val_acc: 0.5869\n",
            "Epoch 212/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2355 - acc: 0.9006 - val_loss: 2.9081 - val_acc: 0.5877\n",
            "Epoch 213/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2375 - acc: 0.8991 - val_loss: 2.9620 - val_acc: 0.5966\n",
            "Epoch 214/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2250 - acc: 0.9010 - val_loss: 2.8579 - val_acc: 0.6003\n",
            "Epoch 215/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2257 - acc: 0.8960 - val_loss: 3.2036 - val_acc: 0.6114\n",
            "Epoch 216/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2285 - acc: 0.8979 - val_loss: 3.2353 - val_acc: 0.6055\n",
            "Epoch 217/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.2376 - acc: 0.8950 - val_loss: 3.1488 - val_acc: 0.6166\n",
            "Epoch 218/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2279 - acc: 0.9016 - val_loss: 3.1012 - val_acc: 0.5996\n",
            "Epoch 219/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2201 - acc: 0.8998 - val_loss: 3.2376 - val_acc: 0.6196\n",
            "Epoch 220/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2247 - acc: 0.8981 - val_loss: 3.3160 - val_acc: 0.6129\n",
            "Epoch 221/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2443 - acc: 0.8937 - val_loss: 2.7893 - val_acc: 0.5936\n",
            "Epoch 222/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.2354 - acc: 0.8956 - val_loss: 3.1726 - val_acc: 0.6092\n",
            "Epoch 223/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2230 - acc: 0.9012 - val_loss: 3.1350 - val_acc: 0.6085\n",
            "Epoch 224/300\n",
            "4809/4809 [==============================] - 1s 132us/step - loss: 0.2237 - acc: 0.9004 - val_loss: 3.1543 - val_acc: 0.6003\n",
            "Epoch 225/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2197 - acc: 0.9029 - val_loss: 3.0707 - val_acc: 0.5944\n",
            "Epoch 226/300\n",
            "4809/4809 [==============================] - 1s 132us/step - loss: 0.2334 - acc: 0.8991 - val_loss: 3.1049 - val_acc: 0.5966\n",
            "Epoch 227/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2276 - acc: 0.8977 - val_loss: 2.9824 - val_acc: 0.5929\n",
            "Epoch 228/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2235 - acc: 0.9070 - val_loss: 3.1144 - val_acc: 0.6070\n",
            "Epoch 229/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2362 - acc: 0.8973 - val_loss: 3.2061 - val_acc: 0.6092\n",
            "Epoch 230/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2214 - acc: 0.9041 - val_loss: 2.7775 - val_acc: 0.5817\n",
            "Epoch 231/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2311 - acc: 0.8983 - val_loss: 3.1042 - val_acc: 0.6085\n",
            "Epoch 232/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2335 - acc: 0.8937 - val_loss: 3.0618 - val_acc: 0.6010\n",
            "Epoch 233/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2369 - acc: 0.9008 - val_loss: 3.0843 - val_acc: 0.5981\n",
            "Epoch 234/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2186 - acc: 0.8996 - val_loss: 3.0671 - val_acc: 0.5936\n",
            "Epoch 235/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.2323 - acc: 0.9016 - val_loss: 3.1766 - val_acc: 0.6070\n",
            "Epoch 236/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2204 - acc: 0.9031 - val_loss: 2.9895 - val_acc: 0.5899\n",
            "Epoch 237/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2173 - acc: 0.9054 - val_loss: 3.0615 - val_acc: 0.5929\n",
            "Epoch 238/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.2410 - acc: 0.8958 - val_loss: 3.0294 - val_acc: 0.5840\n",
            "Epoch 239/300\n",
            "4809/4809 [==============================] - 1s 138us/step - loss: 0.2363 - acc: 0.9025 - val_loss: 3.4026 - val_acc: 0.6077\n",
            "Epoch 240/300\n",
            "4809/4809 [==============================] - 1s 143us/step - loss: 0.2195 - acc: 0.9023 - val_loss: 3.1786 - val_acc: 0.5884\n",
            "Epoch 241/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2188 - acc: 0.9060 - val_loss: 2.8290 - val_acc: 0.5750\n",
            "Epoch 242/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2299 - acc: 0.8948 - val_loss: 3.0936 - val_acc: 0.5906\n",
            "Epoch 243/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2378 - acc: 0.8969 - val_loss: 2.8214 - val_acc: 0.5728\n",
            "Epoch 244/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2360 - acc: 0.8939 - val_loss: 3.2443 - val_acc: 0.5958\n",
            "Epoch 245/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2176 - acc: 0.9070 - val_loss: 3.1905 - val_acc: 0.5906\n",
            "Epoch 246/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2278 - acc: 0.9002 - val_loss: 3.1798 - val_acc: 0.6107\n",
            "Epoch 247/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2184 - acc: 0.9048 - val_loss: 3.2045 - val_acc: 0.6048\n",
            "Epoch 248/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2325 - acc: 0.8994 - val_loss: 3.1705 - val_acc: 0.6062\n",
            "Epoch 249/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2515 - acc: 0.8858 - val_loss: 2.9394 - val_acc: 0.6062\n",
            "Epoch 250/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2334 - acc: 0.8979 - val_loss: 3.1770 - val_acc: 0.6040\n",
            "Epoch 251/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2275 - acc: 0.9019 - val_loss: 3.0600 - val_acc: 0.6003\n",
            "Epoch 252/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.2409 - acc: 0.9048 - val_loss: 2.9575 - val_acc: 0.5869\n",
            "Epoch 253/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2241 - acc: 0.8991 - val_loss: 3.0861 - val_acc: 0.5921\n",
            "Epoch 254/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2228 - acc: 0.9021 - val_loss: 3.3817 - val_acc: 0.6122\n",
            "Epoch 255/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2377 - acc: 0.8987 - val_loss: 3.3130 - val_acc: 0.6174\n",
            "Epoch 256/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2308 - acc: 0.9033 - val_loss: 3.2219 - val_acc: 0.6092\n",
            "Epoch 257/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2286 - acc: 0.8962 - val_loss: 3.2443 - val_acc: 0.6144\n",
            "Epoch 258/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2249 - acc: 0.9033 - val_loss: 3.1127 - val_acc: 0.5944\n",
            "Epoch 259/300\n",
            "4809/4809 [==============================] - 1s 145us/step - loss: 0.2308 - acc: 0.9031 - val_loss: 3.2324 - val_acc: 0.6062\n",
            "Epoch 260/300\n",
            "4809/4809 [==============================] - 1s 148us/step - loss: 0.2329 - acc: 0.9006 - val_loss: 2.8020 - val_acc: 0.5654\n",
            "Epoch 261/300\n",
            "4809/4809 [==============================] - 1s 147us/step - loss: 0.2301 - acc: 0.9016 - val_loss: 3.3407 - val_acc: 0.6248\n",
            "Epoch 262/300\n",
            "4809/4809 [==============================] - 1s 145us/step - loss: 0.2325 - acc: 0.8967 - val_loss: 3.1594 - val_acc: 0.6003\n",
            "Epoch 263/300\n",
            "4809/4809 [==============================] - 1s 148us/step - loss: 0.2293 - acc: 0.9000 - val_loss: 3.1185 - val_acc: 0.5944\n",
            "Epoch 264/300\n",
            "4809/4809 [==============================] - 1s 145us/step - loss: 0.2218 - acc: 0.9008 - val_loss: 3.4421 - val_acc: 0.6159\n",
            "Epoch 265/300\n",
            "4809/4809 [==============================] - 1s 146us/step - loss: 0.2309 - acc: 0.9019 - val_loss: 3.1405 - val_acc: 0.5914\n",
            "Epoch 266/300\n",
            "4809/4809 [==============================] - 1s 145us/step - loss: 0.2235 - acc: 0.9041 - val_loss: 3.1993 - val_acc: 0.6003\n",
            "Epoch 267/300\n",
            "4809/4809 [==============================] - 1s 147us/step - loss: 0.2393 - acc: 0.8981 - val_loss: 3.1598 - val_acc: 0.6018\n",
            "Epoch 268/300\n",
            "4809/4809 [==============================] - 1s 149us/step - loss: 0.2240 - acc: 0.9058 - val_loss: 3.2910 - val_acc: 0.6114\n",
            "Epoch 269/300\n",
            "4809/4809 [==============================] - 1s 146us/step - loss: 0.2489 - acc: 0.8923 - val_loss: 3.1728 - val_acc: 0.6040\n",
            "Epoch 270/300\n",
            "4809/4809 [==============================] - 1s 147us/step - loss: 0.2223 - acc: 0.9023 - val_loss: 3.3122 - val_acc: 0.6025\n",
            "Epoch 271/300\n",
            "4809/4809 [==============================] - 1s 147us/step - loss: 0.2386 - acc: 0.8937 - val_loss: 3.0918 - val_acc: 0.5914\n",
            "Epoch 272/300\n",
            "4809/4809 [==============================] - 1s 145us/step - loss: 0.2322 - acc: 0.8985 - val_loss: 3.2585 - val_acc: 0.6137\n",
            "Epoch 273/300\n",
            "4809/4809 [==============================] - 1s 140us/step - loss: 0.2338 - acc: 0.8971 - val_loss: 3.2994 - val_acc: 0.6256\n",
            "Epoch 274/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2294 - acc: 0.9004 - val_loss: 3.8793 - val_acc: 0.6337\n",
            "Epoch 275/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2257 - acc: 0.9027 - val_loss: 3.0742 - val_acc: 0.5958\n",
            "Epoch 276/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2188 - acc: 0.9079 - val_loss: 3.1214 - val_acc: 0.5906\n",
            "Epoch 277/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.2299 - acc: 0.9035 - val_loss: 3.0598 - val_acc: 0.5981\n",
            "Epoch 278/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2252 - acc: 0.9046 - val_loss: 3.4809 - val_acc: 0.6122\n",
            "Epoch 279/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.2351 - acc: 0.8956 - val_loss: 3.3492 - val_acc: 0.6062\n",
            "Epoch 280/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2278 - acc: 0.9019 - val_loss: 2.9928 - val_acc: 0.5973\n",
            "Epoch 281/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2399 - acc: 0.8964 - val_loss: 3.1139 - val_acc: 0.5944\n",
            "Epoch 282/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2358 - acc: 0.8944 - val_loss: 3.1388 - val_acc: 0.5936\n",
            "Epoch 283/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2273 - acc: 0.9010 - val_loss: 3.3594 - val_acc: 0.5966\n",
            "Epoch 284/300\n",
            "4809/4809 [==============================] - 1s 138us/step - loss: 0.2382 - acc: 0.8952 - val_loss: 3.0224 - val_acc: 0.5832\n",
            "Epoch 285/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2238 - acc: 0.9000 - val_loss: 3.1147 - val_acc: 0.6025\n",
            "Epoch 286/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2241 - acc: 0.9066 - val_loss: 3.2584 - val_acc: 0.6062\n",
            "Epoch 287/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2167 - acc: 0.9043 - val_loss: 3.0896 - val_acc: 0.5921\n",
            "Epoch 288/300\n",
            "4809/4809 [==============================] - 1s 136us/step - loss: 0.2232 - acc: 0.8981 - val_loss: 3.2126 - val_acc: 0.5884\n",
            "Epoch 289/300\n",
            "4809/4809 [==============================] - 1s 132us/step - loss: 0.2285 - acc: 0.9012 - val_loss: 3.1607 - val_acc: 0.5944\n",
            "Epoch 290/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2124 - acc: 0.9087 - val_loss: 3.2572 - val_acc: 0.5944\n",
            "Epoch 291/300\n",
            "4809/4809 [==============================] - 1s 133us/step - loss: 0.2334 - acc: 0.9023 - val_loss: 3.3591 - val_acc: 0.6033\n",
            "Epoch 292/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.2205 - acc: 0.9021 - val_loss: 3.1065 - val_acc: 0.6055\n",
            "Epoch 293/300\n",
            "4809/4809 [==============================] - 1s 137us/step - loss: 0.2200 - acc: 0.9043 - val_loss: 3.2908 - val_acc: 0.6070\n",
            "Epoch 294/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2342 - acc: 0.8987 - val_loss: 3.0573 - val_acc: 0.5869\n",
            "Epoch 295/300\n",
            "4809/4809 [==============================] - 1s 134us/step - loss: 0.2310 - acc: 0.9016 - val_loss: 3.2953 - val_acc: 0.6010\n",
            "Epoch 296/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2270 - acc: 0.8967 - val_loss: 3.1448 - val_acc: 0.5914\n",
            "Epoch 297/300\n",
            "4809/4809 [==============================] - 1s 132us/step - loss: 0.2376 - acc: 0.8933 - val_loss: 3.2144 - val_acc: 0.6077\n",
            "Epoch 298/300\n",
            "4809/4809 [==============================] - 1s 135us/step - loss: 0.2282 - acc: 0.8998 - val_loss: 3.3478 - val_acc: 0.6077\n",
            "Epoch 299/300\n",
            "4809/4809 [==============================] - 1s 132us/step - loss: 0.2318 - acc: 0.8991 - val_loss: 3.3398 - val_acc: 0.6062\n",
            "Epoch 300/300\n",
            "4809/4809 [==============================] - 1s 139us/step - loss: 0.2293 - acc: 0.9006 - val_loss: 3.5922 - val_acc: 0.6181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GPgrZ70JAEJ",
        "colab_type": "text"
      },
      "source": [
        "The training process of this small neural network is very fast : ~2s per epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCtnCbfWJAEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_top.save_weights('/content/drive/My Drive/gpu/bottleneck_300_epochs.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgcNh40EJAEL",
        "colab_type": "text"
      },
      "source": [
        "### Bottleneck model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5JmKK_AJAEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model_top.load_weights('models/bottleneck_30_epochs.h5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVGlFhmNJAEQ",
        "colab_type": "text"
      },
      "source": [
        "Loss and accuracy :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDys9Y2yJAER",
        "colab_type": "code",
        "outputId": "cd67d88f-5ec8-4056-8a6d-6c5eddb870e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model_top.evaluate(validation_data, validation_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1346/1346 [==============================] - 0s 79us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.592292955892339, 0.6181277860326895]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGMsSGj0JAET",
        "colab_type": "text"
      },
      "source": [
        "Evolution of accuracy on training (blue) and validation (green) sets for 1 to 32 epochs :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OrIs4HfJAET",
        "colab_type": "text"
      },
      "source": [
        "![Accuracy evolution](pictures/scores_with_bottleneck.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf6sY0n6JAET",
        "colab_type": "text"
      },
      "source": [
        "**We reached a 90% accuracy on the validation after ~1m of training (~20 epochs) and 8% of the samples originally available on the Kaggle competition !**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg0M3125JAEU",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning the top layers of a a pre-trained network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fk7NUGNJAEU",
        "colab_type": "text"
      },
      "source": [
        "Start by instantiating the VGG base and loading its weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czxweJkEJAEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "##Updated to Keras 2.0\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras import optimizers\n",
        "from keras import applications\n",
        "from keras.models import Model\n",
        "from IPython.display import Image\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJX7pOlOJAEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_dir ='/Users/zhangchong/deep_learning/Open_I_abd_vs_CXRs/TRAIN'\n",
        "validation_data_dir ='/Users/zhangchong/deep_learning/Open_I_abd_vs_CXRs/VAL'\n",
        "train_samples=65\n",
        "validation_samples=10\n",
        "img_width, img_height = 150, 150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saEBBrRLJAEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_vgg=applications.VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MONVplrzJAEg",
        "colab_type": "text"
      },
      "source": [
        "Build a classifier model to put on top of the convolutional model. For the fine tuning, we start with a fully trained-classifer. We will use the weights from the earlier model. And then we will add this model on top of the convolutional base."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6wXXbhoJAEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_top=Sequential()\n",
        "model_top.add(Flatten(input_shape=model_vgg.output_shape[1:]))\n",
        "model_top.add(Dense(512,activation='relu'))\n",
        "model_top.add(Dropout(0.5))\n",
        "model_top.add(Dense(256,activation='relu'))\n",
        "model_top.add(Dropout(0.5))\n",
        "model_top.add(Dense(1,activation='sigmoid'))\n",
        "model_top.load_weights('/content/drive/My Drive/gpu/bottleneck_300_epochs.h5')\n",
        "#model_vgg.add(top_model)\n",
        "model = Model(inputs = model_vgg.input, outputs = model_top(model_vgg.output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2wuQttzJAEj",
        "colab_type": "code",
        "outputId": "82b17132-7a1f-456c-9698-87b79839adb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "sequential_11 (Sequential)   (None, 1)                 4326401   \n",
            "=================================================================\n",
            "Total params: 19,041,089\n",
            "Trainable params: 11,405,825\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcYb9bYcJAEl",
        "colab_type": "text"
      },
      "source": [
        "For fine turning, we only want to train a few layers.  This line will set the first 25 layers (up to the conv block) to non-trainable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gzgiw1cJAEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in model_vgg.layers[:15]:\n",
        "    layer.trainable = False\n",
        "# for layer in model_v3.layers:\n",
        "#     layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJnz6fHkJAEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile the model with a SGD/momentum optimizer\n",
        "# and a very slow learning rate.\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "# model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9ymSQHHJAEr",
        "colab_type": "code",
        "outputId": "fd874104-2719-44d3-e63c-50ff95a23416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "sequential_7 (Sequential)    (None, 1)                 131585    \n",
            "=================================================================\n",
            "Total params: 14,846,273\n",
            "Trainable params: 7,211,009\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmspMsCmO5rd",
        "colab_type": "code",
        "outputId": "3e3216fc-9e7b-43e8-fc09-5d2ec13d63e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_label.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4809,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wvg1tQ6Oq_r",
        "colab_type": "code",
        "outputId": "c97fb134-1eea-4ba8-f382-9e233d15016b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10254
        }
      },
      "source": [
        "batch_size=5\n",
        "model_histoty=model.fit(train_3chanel,train_label,validation_data=(test_3chanel,inval_label),verbose=1,epochs=300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4809 samples, validate on 1346 samples\n",
            "Epoch 1/300\n",
            "4809/4809 [==============================] - 13s 3ms/step - loss: 0.7887 - acc: 0.4151 - val_loss: 0.6498 - val_acc: 0.6538\n",
            "Epoch 2/300\n",
            "4809/4809 [==============================] - 13s 3ms/step - loss: 0.6947 - acc: 0.5725 - val_loss: 0.6484 - val_acc: 0.6538\n",
            "Epoch 3/300\n",
            "4809/4809 [==============================] - 13s 3ms/step - loss: 0.6830 - acc: 0.5953 - val_loss: 0.6469 - val_acc: 0.6538\n",
            "Epoch 4/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6819 - acc: 0.5904 - val_loss: 0.6456 - val_acc: 0.6538\n",
            "Epoch 5/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6779 - acc: 0.6014 - val_loss: 0.6479 - val_acc: 0.6538\n",
            "Epoch 6/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6744 - acc: 0.6045 - val_loss: 0.6455 - val_acc: 0.6538\n",
            "Epoch 7/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6721 - acc: 0.6003 - val_loss: 0.6445 - val_acc: 0.6538\n",
            "Epoch 8/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6705 - acc: 0.6051 - val_loss: 0.6442 - val_acc: 0.6538\n",
            "Epoch 9/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6714 - acc: 0.6134 - val_loss: 0.6435 - val_acc: 0.6538\n",
            "Epoch 10/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6596 - acc: 0.6234 - val_loss: 0.6408 - val_acc: 0.6538\n",
            "Epoch 11/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6561 - acc: 0.6384 - val_loss: 0.6393 - val_acc: 0.6538\n",
            "Epoch 12/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6637 - acc: 0.6190 - val_loss: 0.6432 - val_acc: 0.6538\n",
            "Epoch 13/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6535 - acc: 0.6332 - val_loss: 0.6375 - val_acc: 0.6538\n",
            "Epoch 14/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6517 - acc: 0.6388 - val_loss: 0.6364 - val_acc: 0.6538\n",
            "Epoch 15/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6499 - acc: 0.6373 - val_loss: 0.6367 - val_acc: 0.6538\n",
            "Epoch 16/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6448 - acc: 0.6440 - val_loss: 0.6320 - val_acc: 0.6538\n",
            "Epoch 17/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6449 - acc: 0.6463 - val_loss: 0.6320 - val_acc: 0.6538\n",
            "Epoch 18/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6381 - acc: 0.6492 - val_loss: 0.6292 - val_acc: 0.6516\n",
            "Epoch 19/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6369 - acc: 0.6471 - val_loss: 0.6296 - val_acc: 0.6478\n",
            "Epoch 20/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6404 - acc: 0.6463 - val_loss: 0.6318 - val_acc: 0.6404\n",
            "Epoch 21/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6341 - acc: 0.6488 - val_loss: 0.6289 - val_acc: 0.6419\n",
            "Epoch 22/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6313 - acc: 0.6548 - val_loss: 0.6282 - val_acc: 0.6449\n",
            "Epoch 23/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6295 - acc: 0.6573 - val_loss: 0.6263 - val_acc: 0.6471\n",
            "Epoch 24/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6272 - acc: 0.6540 - val_loss: 0.6263 - val_acc: 0.6471\n",
            "Epoch 25/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6285 - acc: 0.6544 - val_loss: 0.6266 - val_acc: 0.6419\n",
            "Epoch 26/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6244 - acc: 0.6538 - val_loss: 0.6296 - val_acc: 0.6412\n",
            "Epoch 27/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6154 - acc: 0.6600 - val_loss: 0.6301 - val_acc: 0.6389\n",
            "Epoch 28/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6131 - acc: 0.6638 - val_loss: 0.6297 - val_acc: 0.6419\n",
            "Epoch 29/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6151 - acc: 0.6598 - val_loss: 0.6309 - val_acc: 0.6345\n",
            "Epoch 30/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6110 - acc: 0.6638 - val_loss: 0.6292 - val_acc: 0.6464\n",
            "Epoch 31/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6094 - acc: 0.6579 - val_loss: 0.6300 - val_acc: 0.6471\n",
            "Epoch 32/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6041 - acc: 0.6727 - val_loss: 0.6343 - val_acc: 0.6330\n",
            "Epoch 33/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.6038 - acc: 0.6742 - val_loss: 0.6350 - val_acc: 0.6441\n",
            "Epoch 34/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5986 - acc: 0.6698 - val_loss: 0.6363 - val_acc: 0.6434\n",
            "Epoch 35/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5978 - acc: 0.6714 - val_loss: 0.6402 - val_acc: 0.6426\n",
            "Epoch 36/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5911 - acc: 0.6802 - val_loss: 0.6431 - val_acc: 0.6248\n",
            "Epoch 37/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5907 - acc: 0.6714 - val_loss: 0.6460 - val_acc: 0.6107\n",
            "Epoch 38/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5813 - acc: 0.6893 - val_loss: 0.6443 - val_acc: 0.6404\n",
            "Epoch 39/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5879 - acc: 0.6783 - val_loss: 0.6496 - val_acc: 0.6389\n",
            "Epoch 40/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5732 - acc: 0.6875 - val_loss: 0.6555 - val_acc: 0.6085\n",
            "Epoch 41/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5783 - acc: 0.6833 - val_loss: 0.6575 - val_acc: 0.6345\n",
            "Epoch 42/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5716 - acc: 0.6823 - val_loss: 0.6740 - val_acc: 0.6449\n",
            "Epoch 43/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5659 - acc: 0.6891 - val_loss: 0.6678 - val_acc: 0.5869\n",
            "Epoch 44/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5649 - acc: 0.6935 - val_loss: 0.6689 - val_acc: 0.6382\n",
            "Epoch 45/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5595 - acc: 0.6920 - val_loss: 0.6681 - val_acc: 0.6070\n",
            "Epoch 46/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5585 - acc: 0.6949 - val_loss: 0.6743 - val_acc: 0.6025\n",
            "Epoch 47/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5495 - acc: 0.7018 - val_loss: 0.6738 - val_acc: 0.6114\n",
            "Epoch 48/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5485 - acc: 0.7080 - val_loss: 0.6932 - val_acc: 0.6486\n",
            "Epoch 49/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5441 - acc: 0.7141 - val_loss: 0.6819 - val_acc: 0.6003\n",
            "Epoch 50/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5427 - acc: 0.7101 - val_loss: 0.6904 - val_acc: 0.6085\n",
            "Epoch 51/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5323 - acc: 0.7128 - val_loss: 0.7134 - val_acc: 0.6441\n",
            "Epoch 52/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5295 - acc: 0.7193 - val_loss: 0.6960 - val_acc: 0.6181\n",
            "Epoch 53/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5255 - acc: 0.7274 - val_loss: 0.7130 - val_acc: 0.6010\n",
            "Epoch 54/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5190 - acc: 0.7218 - val_loss: 0.7047 - val_acc: 0.6077\n",
            "Epoch 55/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5229 - acc: 0.7301 - val_loss: 0.7154 - val_acc: 0.5996\n",
            "Epoch 56/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5084 - acc: 0.7394 - val_loss: 0.7130 - val_acc: 0.5698\n",
            "Epoch 57/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.5059 - acc: 0.7436 - val_loss: 0.7568 - val_acc: 0.6404\n",
            "Epoch 58/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4953 - acc: 0.7538 - val_loss: 0.7515 - val_acc: 0.6196\n",
            "Epoch 59/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4865 - acc: 0.7555 - val_loss: 0.7548 - val_acc: 0.6129\n",
            "Epoch 60/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4886 - acc: 0.7544 - val_loss: 0.7602 - val_acc: 0.6129\n",
            "Epoch 61/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4849 - acc: 0.7636 - val_loss: 0.7402 - val_acc: 0.5305\n",
            "Epoch 62/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4778 - acc: 0.7638 - val_loss: 0.8064 - val_acc: 0.6129\n",
            "Epoch 63/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4834 - acc: 0.7557 - val_loss: 0.7366 - val_acc: 0.4874\n",
            "Epoch 64/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4760 - acc: 0.7602 - val_loss: 0.7561 - val_acc: 0.5572\n",
            "Epoch 65/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4678 - acc: 0.7665 - val_loss: 0.7820 - val_acc: 0.6070\n",
            "Epoch 66/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4619 - acc: 0.7717 - val_loss: 0.7511 - val_acc: 0.5290\n",
            "Epoch 67/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4701 - acc: 0.7600 - val_loss: 0.8059 - val_acc: 0.6270\n",
            "Epoch 68/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4563 - acc: 0.7806 - val_loss: 0.8111 - val_acc: 0.6129\n",
            "Epoch 69/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4425 - acc: 0.7873 - val_loss: 0.8408 - val_acc: 0.6248\n",
            "Epoch 70/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4419 - acc: 0.7954 - val_loss: 0.8077 - val_acc: 0.5498\n",
            "Epoch 71/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4258 - acc: 0.8018 - val_loss: 0.8108 - val_acc: 0.6107\n",
            "Epoch 72/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4273 - acc: 0.8066 - val_loss: 0.7832 - val_acc: 0.5527\n",
            "Epoch 73/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4164 - acc: 0.8122 - val_loss: 0.8727 - val_acc: 0.6010\n",
            "Epoch 74/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4357 - acc: 0.7939 - val_loss: 0.8802 - val_acc: 0.6122\n",
            "Epoch 75/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4157 - acc: 0.8216 - val_loss: 0.8348 - val_acc: 0.5862\n",
            "Epoch 76/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4296 - acc: 0.7964 - val_loss: 0.8060 - val_acc: 0.4681\n",
            "Epoch 77/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.4041 - acc: 0.8228 - val_loss: 1.0333 - val_acc: 0.6464\n",
            "Epoch 78/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3980 - acc: 0.8347 - val_loss: 0.8446 - val_acc: 0.5906\n",
            "Epoch 79/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3982 - acc: 0.8237 - val_loss: 0.8432 - val_acc: 0.5579\n",
            "Epoch 80/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3911 - acc: 0.8278 - val_loss: 0.8942 - val_acc: 0.5929\n",
            "Epoch 81/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3750 - acc: 0.8424 - val_loss: 0.8394 - val_acc: 0.5550\n",
            "Epoch 82/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3655 - acc: 0.8511 - val_loss: 0.9050 - val_acc: 0.5758\n",
            "Epoch 83/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3650 - acc: 0.8499 - val_loss: 0.9279 - val_acc: 0.5847\n",
            "Epoch 84/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3910 - acc: 0.8282 - val_loss: 0.9367 - val_acc: 0.5921\n",
            "Epoch 85/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3624 - acc: 0.8532 - val_loss: 0.9500 - val_acc: 0.5788\n",
            "Epoch 86/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3363 - acc: 0.8696 - val_loss: 0.9093 - val_acc: 0.5587\n",
            "Epoch 87/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3442 - acc: 0.8684 - val_loss: 1.1352 - val_acc: 0.6256\n",
            "Epoch 88/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3360 - acc: 0.8773 - val_loss: 1.0488 - val_acc: 0.5877\n",
            "Epoch 89/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3796 - acc: 0.8301 - val_loss: 0.9653 - val_acc: 0.5810\n",
            "Epoch 90/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3626 - acc: 0.8497 - val_loss: 1.2523 - val_acc: 0.6441\n",
            "Epoch 91/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3327 - acc: 0.8748 - val_loss: 1.1742 - val_acc: 0.6218\n",
            "Epoch 92/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3239 - acc: 0.8779 - val_loss: 0.9248 - val_acc: 0.5565\n",
            "Epoch 93/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3198 - acc: 0.8813 - val_loss: 1.0968 - val_acc: 0.6018\n",
            "Epoch 94/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3269 - acc: 0.8767 - val_loss: 1.0412 - val_acc: 0.5936\n",
            "Epoch 95/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3066 - acc: 0.8860 - val_loss: 1.0349 - val_acc: 0.6055\n",
            "Epoch 96/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3053 - acc: 0.8867 - val_loss: 1.0208 - val_acc: 0.5386\n",
            "Epoch 97/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3076 - acc: 0.8854 - val_loss: 1.3684 - val_acc: 0.6389\n",
            "Epoch 98/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.3015 - acc: 0.8890 - val_loss: 1.1251 - val_acc: 0.5951\n",
            "Epoch 99/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2720 - acc: 0.9089 - val_loss: 0.9389 - val_acc: 0.5193\n",
            "Epoch 100/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2925 - acc: 0.8964 - val_loss: 1.2694 - val_acc: 0.6174\n",
            "Epoch 101/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2941 - acc: 0.8858 - val_loss: 1.3187 - val_acc: 0.6241\n",
            "Epoch 102/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2592 - acc: 0.9179 - val_loss: 1.5396 - val_acc: 0.6367\n",
            "Epoch 103/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2579 - acc: 0.9125 - val_loss: 1.2588 - val_acc: 0.5966\n",
            "Epoch 104/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2771 - acc: 0.9019 - val_loss: 1.4273 - val_acc: 0.6226\n",
            "Epoch 105/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2491 - acc: 0.9231 - val_loss: 1.2981 - val_acc: 0.6100\n",
            "Epoch 106/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2370 - acc: 0.9295 - val_loss: 1.1554 - val_acc: 0.5490\n",
            "Epoch 107/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2296 - acc: 0.9330 - val_loss: 1.4719 - val_acc: 0.6189\n",
            "Epoch 108/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2242 - acc: 0.9395 - val_loss: 1.4537 - val_acc: 0.6233\n",
            "Epoch 109/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2469 - acc: 0.9141 - val_loss: 1.3936 - val_acc: 0.6077\n",
            "Epoch 110/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2361 - acc: 0.9256 - val_loss: 1.4009 - val_acc: 0.6025\n",
            "Epoch 111/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2704 - acc: 0.9033 - val_loss: 1.6068 - val_acc: 0.6412\n",
            "Epoch 112/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2265 - acc: 0.9310 - val_loss: 1.4527 - val_acc: 0.6010\n",
            "Epoch 113/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2261 - acc: 0.9314 - val_loss: 1.3113 - val_acc: 0.5936\n",
            "Epoch 114/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2093 - acc: 0.9416 - val_loss: 1.4580 - val_acc: 0.6070\n",
            "Epoch 115/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2047 - acc: 0.9441 - val_loss: 1.7219 - val_acc: 0.6352\n",
            "Epoch 116/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2057 - acc: 0.9378 - val_loss: 1.5534 - val_acc: 0.6211\n",
            "Epoch 117/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1847 - acc: 0.9549 - val_loss: 1.1116 - val_acc: 0.5082\n",
            "Epoch 118/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1941 - acc: 0.9464 - val_loss: 1.9459 - val_acc: 0.6434\n",
            "Epoch 119/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2075 - acc: 0.9374 - val_loss: 1.8279 - val_acc: 0.6278\n",
            "Epoch 120/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2088 - acc: 0.9343 - val_loss: 2.2022 - val_acc: 0.6486\n",
            "Epoch 121/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1900 - acc: 0.9480 - val_loss: 1.7061 - val_acc: 0.6152\n",
            "Epoch 122/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1699 - acc: 0.9599 - val_loss: 1.4475 - val_acc: 0.5832\n",
            "Epoch 123/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.2112 - acc: 0.9295 - val_loss: 1.7326 - val_acc: 0.6181\n",
            "Epoch 124/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1670 - acc: 0.9563 - val_loss: 1.9635 - val_acc: 0.6345\n",
            "Epoch 125/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1649 - acc: 0.9580 - val_loss: 1.9645 - val_acc: 0.6270\n",
            "Epoch 126/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1662 - acc: 0.9532 - val_loss: 1.8583 - val_acc: 0.6129\n",
            "Epoch 127/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1858 - acc: 0.9397 - val_loss: 2.2248 - val_acc: 0.6337\n",
            "Epoch 128/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1702 - acc: 0.9534 - val_loss: 2.0295 - val_acc: 0.6397\n",
            "Epoch 129/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1439 - acc: 0.9636 - val_loss: 1.8045 - val_acc: 0.6070\n",
            "Epoch 130/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1583 - acc: 0.9505 - val_loss: 1.6318 - val_acc: 0.5572\n",
            "Epoch 131/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1823 - acc: 0.9397 - val_loss: 1.9070 - val_acc: 0.6040\n",
            "Epoch 132/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1275 - acc: 0.9721 - val_loss: 1.9312 - val_acc: 0.6233\n",
            "Epoch 133/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1628 - acc: 0.9472 - val_loss: 1.8123 - val_acc: 0.5825\n",
            "Epoch 134/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1392 - acc: 0.9636 - val_loss: 1.6969 - val_acc: 0.5773\n",
            "Epoch 135/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1449 - acc: 0.9557 - val_loss: 2.5765 - val_acc: 0.6389\n",
            "Epoch 136/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1407 - acc: 0.9574 - val_loss: 1.7368 - val_acc: 0.5557\n",
            "Epoch 137/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1406 - acc: 0.9586 - val_loss: 1.9478 - val_acc: 0.6048\n",
            "Epoch 138/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1127 - acc: 0.9653 - val_loss: 1.8069 - val_acc: 0.5691\n",
            "Epoch 139/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1525 - acc: 0.9518 - val_loss: 1.8068 - val_acc: 0.5877\n",
            "Epoch 140/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1304 - acc: 0.9624 - val_loss: 2.0655 - val_acc: 0.6270\n",
            "Epoch 141/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1139 - acc: 0.9709 - val_loss: 2.3661 - val_acc: 0.6315\n",
            "Epoch 142/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1893 - acc: 0.9295 - val_loss: 2.2328 - val_acc: 0.6166\n",
            "Epoch 143/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1075 - acc: 0.9678 - val_loss: 1.7895 - val_acc: 0.5817\n",
            "Epoch 144/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1745 - acc: 0.9399 - val_loss: 1.8876 - val_acc: 0.6174\n",
            "Epoch 145/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0963 - acc: 0.9771 - val_loss: 2.6514 - val_acc: 0.6241\n",
            "Epoch 146/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0993 - acc: 0.9715 - val_loss: 1.9472 - val_acc: 0.5869\n",
            "Epoch 147/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0930 - acc: 0.9707 - val_loss: 2.0507 - val_acc: 0.5676\n",
            "Epoch 148/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1090 - acc: 0.9674 - val_loss: 2.3534 - val_acc: 0.6218\n",
            "Epoch 149/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1128 - acc: 0.9607 - val_loss: 1.9340 - val_acc: 0.5958\n",
            "Epoch 150/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0732 - acc: 0.9796 - val_loss: 2.8115 - val_acc: 0.6241\n",
            "Epoch 151/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1408 - acc: 0.9553 - val_loss: 2.4239 - val_acc: 0.6471\n",
            "Epoch 152/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1027 - acc: 0.9786 - val_loss: 1.8120 - val_acc: 0.6025\n",
            "Epoch 153/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0806 - acc: 0.9884 - val_loss: 1.7478 - val_acc: 0.5929\n",
            "Epoch 154/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0739 - acc: 0.9900 - val_loss: 2.0105 - val_acc: 0.6129\n",
            "Epoch 155/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0760 - acc: 0.9873 - val_loss: 1.8077 - val_acc: 0.5869\n",
            "Epoch 156/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0720 - acc: 0.9850 - val_loss: 2.4209 - val_acc: 0.6159\n",
            "Epoch 157/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0713 - acc: 0.9771 - val_loss: 2.9619 - val_acc: 0.6434\n",
            "Epoch 158/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0774 - acc: 0.9755 - val_loss: 2.1804 - val_acc: 0.6166\n",
            "Epoch 159/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0630 - acc: 0.9896 - val_loss: 3.1544 - val_acc: 0.5089\n",
            "Epoch 160/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1306 - acc: 0.9540 - val_loss: 2.8836 - val_acc: 0.5958\n",
            "Epoch 161/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1179 - acc: 0.9595 - val_loss: 2.3893 - val_acc: 0.5795\n",
            "Epoch 162/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0586 - acc: 0.9825 - val_loss: 2.8543 - val_acc: 0.5290\n",
            "Epoch 163/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0654 - acc: 0.9800 - val_loss: 4.1860 - val_acc: 0.6545\n",
            "Epoch 164/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1944 - acc: 0.9457 - val_loss: 1.7805 - val_acc: 0.5966\n",
            "Epoch 165/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0695 - acc: 0.9900 - val_loss: 1.9399 - val_acc: 0.6003\n",
            "Epoch 166/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0607 - acc: 0.9913 - val_loss: 2.1517 - val_acc: 0.6137\n",
            "Epoch 167/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0632 - acc: 0.9886 - val_loss: 2.4080 - val_acc: 0.6040\n",
            "Epoch 168/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0518 - acc: 0.9946 - val_loss: 2.1680 - val_acc: 0.6070\n",
            "Epoch 169/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0504 - acc: 0.9923 - val_loss: 2.3231 - val_acc: 0.6055\n",
            "Epoch 170/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0437 - acc: 0.9960 - val_loss: 2.7598 - val_acc: 0.6308\n",
            "Epoch 171/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0440 - acc: 0.9938 - val_loss: 2.4812 - val_acc: 0.6166\n",
            "Epoch 172/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0596 - acc: 0.9854 - val_loss: 2.1210 - val_acc: 0.5892\n",
            "Epoch 173/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0406 - acc: 0.9938 - val_loss: 2.6841 - val_acc: 0.6114\n",
            "Epoch 174/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0337 - acc: 0.9971 - val_loss: 2.5997 - val_acc: 0.6122\n",
            "Epoch 175/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0333 - acc: 0.9967 - val_loss: 2.6182 - val_acc: 0.6174\n",
            "Epoch 176/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0538 - acc: 0.9881 - val_loss: 2.3967 - val_acc: 0.6114\n",
            "Epoch 177/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0382 - acc: 0.9950 - val_loss: 2.5505 - val_acc: 0.6122\n",
            "Epoch 178/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0452 - acc: 0.9898 - val_loss: 2.5148 - val_acc: 0.6062\n",
            "Epoch 179/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0317 - acc: 0.9956 - val_loss: 2.5764 - val_acc: 0.6226\n",
            "Epoch 180/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0271 - acc: 0.9971 - val_loss: 2.9754 - val_acc: 0.6248\n",
            "Epoch 181/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0285 - acc: 0.9963 - val_loss: 2.5615 - val_acc: 0.6055\n",
            "Epoch 182/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0286 - acc: 0.9946 - val_loss: 2.5504 - val_acc: 0.6033\n",
            "Epoch 183/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0263 - acc: 0.9960 - val_loss: 2.7072 - val_acc: 0.6204\n",
            "Epoch 184/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0281 - acc: 0.9950 - val_loss: 2.6164 - val_acc: 0.6152\n",
            "Epoch 185/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0270 - acc: 0.9963 - val_loss: 3.2831 - val_acc: 0.6300\n",
            "Epoch 186/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0489 - acc: 0.9877 - val_loss: 2.6319 - val_acc: 0.6033\n",
            "Epoch 187/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0245 - acc: 0.9979 - val_loss: 2.6568 - val_acc: 0.6040\n",
            "Epoch 188/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0221 - acc: 0.9967 - val_loss: 2.9596 - val_acc: 0.6196\n",
            "Epoch 189/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0177 - acc: 0.9983 - val_loss: 2.7339 - val_acc: 0.5929\n",
            "Epoch 190/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0157 - acc: 0.9969 - val_loss: 3.4899 - val_acc: 0.6211\n",
            "Epoch 191/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1294 - acc: 0.9597 - val_loss: 2.0653 - val_acc: 0.5468\n",
            "Epoch 192/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0621 - acc: 0.9844 - val_loss: 2.8528 - val_acc: 0.6248\n",
            "Epoch 193/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0256 - acc: 0.9981 - val_loss: 2.5295 - val_acc: 0.6100\n",
            "Epoch 194/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0230 - acc: 0.9985 - val_loss: 2.9923 - val_acc: 0.6196\n",
            "Epoch 195/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0202 - acc: 0.9985 - val_loss: 2.6994 - val_acc: 0.6137\n",
            "Epoch 196/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0195 - acc: 0.9981 - val_loss: 3.1332 - val_acc: 0.6300\n",
            "Epoch 197/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0216 - acc: 0.9975 - val_loss: 2.6672 - val_acc: 0.6092\n",
            "Epoch 198/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0218 - acc: 0.9969 - val_loss: 2.9027 - val_acc: 0.6166\n",
            "Epoch 199/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0201 - acc: 0.9971 - val_loss: 2.9881 - val_acc: 0.6114\n",
            "Epoch 200/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0286 - acc: 0.9927 - val_loss: 2.8242 - val_acc: 0.6085\n",
            "Epoch 201/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0114 - acc: 0.9983 - val_loss: 3.7633 - val_acc: 0.6100\n",
            "Epoch 202/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0308 - acc: 0.9919 - val_loss: 3.7236 - val_acc: 0.6077\n",
            "Epoch 203/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0145 - acc: 0.9971 - val_loss: 3.9168 - val_acc: 0.5884\n",
            "Epoch 204/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0493 - acc: 0.9869 - val_loss: 3.3357 - val_acc: 0.6196\n",
            "Epoch 205/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0158 - acc: 0.9967 - val_loss: 3.5954 - val_acc: 0.6122\n",
            "Epoch 206/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0480 - acc: 0.9886 - val_loss: 2.6433 - val_acc: 0.5869\n",
            "Epoch 207/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0110 - acc: 0.9994 - val_loss: 3.5773 - val_acc: 0.6025\n",
            "Epoch 208/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0116 - acc: 0.9979 - val_loss: 3.9398 - val_acc: 0.5966\n",
            "Epoch 209/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0874 - acc: 0.9726 - val_loss: 2.6323 - val_acc: 0.5944\n",
            "Epoch 210/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0219 - acc: 0.9946 - val_loss: 3.2681 - val_acc: 0.6293\n",
            "Epoch 211/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0307 - acc: 0.9915 - val_loss: 3.3437 - val_acc: 0.6159\n",
            "Epoch 212/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0064 - acc: 0.9998 - val_loss: 3.8124 - val_acc: 0.6040\n",
            "Epoch 213/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0056 - acc: 0.9990 - val_loss: 4.1034 - val_acc: 0.5899\n",
            "Epoch 214/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0087 - acc: 0.9983 - val_loss: 4.0418 - val_acc: 0.5869\n",
            "Epoch 215/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0490 - acc: 0.9867 - val_loss: 2.5904 - val_acc: 0.6114\n",
            "Epoch 216/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0523 - acc: 0.9863 - val_loss: 2.7696 - val_acc: 0.6040\n",
            "Epoch 217/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0131 - acc: 0.9979 - val_loss: 3.5766 - val_acc: 0.6040\n",
            "Epoch 218/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0633 - acc: 0.9811 - val_loss: 2.9932 - val_acc: 0.6100\n",
            "Epoch 219/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0206 - acc: 0.9985 - val_loss: 2.9181 - val_acc: 0.6129\n",
            "Epoch 220/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0150 - acc: 0.9996 - val_loss: 2.8072 - val_acc: 0.5981\n",
            "Epoch 221/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0136 - acc: 0.9992 - val_loss: 3.1433 - val_acc: 0.6211\n",
            "Epoch 222/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0127 - acc: 0.9992 - val_loss: 3.1788 - val_acc: 0.6181\n",
            "Epoch 223/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0136 - acc: 0.9985 - val_loss: 3.1748 - val_acc: 0.6062\n",
            "Epoch 224/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0113 - acc: 0.9994 - val_loss: 3.0047 - val_acc: 0.6055\n",
            "Epoch 225/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0098 - acc: 0.9994 - val_loss: 3.2120 - val_acc: 0.6137\n",
            "Epoch 226/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0086 - acc: 0.9990 - val_loss: 3.2621 - val_acc: 0.6048\n",
            "Epoch 227/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0050 - acc: 0.9998 - val_loss: 3.6739 - val_acc: 0.6107\n",
            "Epoch 228/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0079 - acc: 0.9977 - val_loss: 4.3240 - val_acc: 0.6018\n",
            "Epoch 229/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0243 - acc: 0.9923 - val_loss: 3.7153 - val_acc: 0.6137\n",
            "Epoch 230/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0186 - acc: 0.9950 - val_loss: 3.7136 - val_acc: 0.6033\n",
            "Epoch 231/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.1280 - acc: 0.9696 - val_loss: 2.9110 - val_acc: 0.6100\n",
            "Epoch 232/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0173 - acc: 0.9988 - val_loss: 2.9328 - val_acc: 0.6122\n",
            "Epoch 233/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0144 - acc: 0.9992 - val_loss: 3.0650 - val_acc: 0.6070\n",
            "Epoch 234/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0131 - acc: 0.9990 - val_loss: 3.2398 - val_acc: 0.6181\n",
            "Epoch 235/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0105 - acc: 0.9996 - val_loss: 3.0112 - val_acc: 0.6048\n",
            "Epoch 236/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0123 - acc: 0.9988 - val_loss: 3.0038 - val_acc: 0.6048\n",
            "Epoch 237/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0100 - acc: 0.9990 - val_loss: 3.0884 - val_acc: 0.6077\n",
            "Epoch 238/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0109 - acc: 0.9988 - val_loss: 3.3481 - val_acc: 0.6241\n",
            "Epoch 239/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0099 - acc: 0.9996 - val_loss: 3.2407 - val_acc: 0.6144\n",
            "Epoch 240/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0387 - acc: 0.9894 - val_loss: 3.4029 - val_acc: 0.6293\n",
            "Epoch 241/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0140 - acc: 0.9981 - val_loss: 3.3534 - val_acc: 0.6285\n",
            "Epoch 242/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0086 - acc: 0.9998 - val_loss: 3.1702 - val_acc: 0.6107\n",
            "Epoch 243/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0085 - acc: 0.9998 - val_loss: 3.3114 - val_acc: 0.6166\n",
            "Epoch 244/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0099 - acc: 0.9985 - val_loss: 3.4582 - val_acc: 0.6270\n",
            "Epoch 245/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0102 - acc: 0.9988 - val_loss: 3.5088 - val_acc: 0.6241\n",
            "Epoch 246/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0078 - acc: 0.9996 - val_loss: 3.3442 - val_acc: 0.6256\n",
            "Epoch 247/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0071 - acc: 0.9994 - val_loss: 3.3322 - val_acc: 0.6345\n",
            "Epoch 248/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.4045 - val_acc: 0.6330\n",
            "Epoch 249/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0062 - acc: 0.9998 - val_loss: 3.1982 - val_acc: 0.6040\n",
            "Epoch 250/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0063 - acc: 0.9996 - val_loss: 3.2985 - val_acc: 0.6040\n",
            "Epoch 251/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0043 - acc: 0.9998 - val_loss: 3.5545 - val_acc: 0.6189\n",
            "Epoch 252/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0037 - acc: 0.9998 - val_loss: 3.7353 - val_acc: 0.6233\n",
            "Epoch 253/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0038 - acc: 0.9992 - val_loss: 4.1623 - val_acc: 0.6033\n",
            "Epoch 254/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0605 - acc: 0.9888 - val_loss: 3.5915 - val_acc: 0.6211\n",
            "Epoch 255/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0410 - acc: 0.9879 - val_loss: 3.0454 - val_acc: 0.6010\n",
            "Epoch 256/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0090 - acc: 0.9996 - val_loss: 3.3529 - val_acc: 0.6308\n",
            "Epoch 257/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0049 - acc: 0.9994 - val_loss: 3.7063 - val_acc: 0.6181\n",
            "Epoch 258/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0026 - acc: 0.9998 - val_loss: 4.0111 - val_acc: 0.6159\n",
            "Epoch 259/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 4.1870 - val_acc: 0.6189\n",
            "Epoch 260/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 4.2622 - val_acc: 0.5973\n",
            "Epoch 261/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 4.2353 - val_acc: 0.6211\n",
            "Epoch 262/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0022 - acc: 0.9998 - val_loss: 4.3157 - val_acc: 0.6181\n",
            "Epoch 263/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 4.3366 - val_acc: 0.6144\n",
            "Epoch 264/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 4.3962 - val_acc: 0.6107\n",
            "Epoch 265/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0160 - acc: 0.9942 - val_loss: 3.7766 - val_acc: 0.6137\n",
            "Epoch 266/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0035 - acc: 0.9998 - val_loss: 4.0308 - val_acc: 0.6159\n",
            "Epoch 267/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0021 - acc: 0.9998 - val_loss: 4.1371 - val_acc: 0.6189\n",
            "Epoch 268/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0019 - acc: 0.9998 - val_loss: 4.2423 - val_acc: 0.6144\n",
            "Epoch 269/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 4.3366 - val_acc: 0.6166\n",
            "Epoch 270/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 4.4384 - val_acc: 0.6048\n",
            "Epoch 271/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0405 - acc: 0.9892 - val_loss: 3.0039 - val_acc: 0.5929\n",
            "Epoch 272/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0123 - acc: 0.9981 - val_loss: 3.2567 - val_acc: 0.6100\n",
            "Epoch 273/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0048 - acc: 0.9998 - val_loss: 3.5955 - val_acc: 0.6122\n",
            "Epoch 274/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0051 - acc: 0.9988 - val_loss: 3.7358 - val_acc: 0.6107\n",
            "Epoch 275/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0034 - acc: 0.9998 - val_loss: 3.9788 - val_acc: 0.6107\n",
            "Epoch 276/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0461 - acc: 0.9886 - val_loss: 3.1580 - val_acc: 0.6174\n",
            "Epoch 277/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0163 - acc: 0.9960 - val_loss: 3.2693 - val_acc: 0.5951\n",
            "Epoch 278/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.7257 - val_acc: 0.6048\n",
            "Epoch 279/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0061 - acc: 0.9996 - val_loss: 3.9848 - val_acc: 0.6077\n",
            "Epoch 280/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 4.2431 - val_acc: 0.6048\n",
            "Epoch 281/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 4.3026 - val_acc: 0.6122\n",
            "Epoch 282/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 4.4066 - val_acc: 0.6100\n",
            "Epoch 283/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 4.4908 - val_acc: 0.6122\n",
            "Epoch 284/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0023 - acc: 0.9998 - val_loss: 4.4972 - val_acc: 0.6048\n",
            "Epoch 285/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 4.4800 - val_acc: 0.6077\n",
            "Epoch 286/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0022 - acc: 0.9998 - val_loss: 4.4791 - val_acc: 0.6114\n",
            "Epoch 287/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 4.5721 - val_acc: 0.6085\n",
            "Epoch 288/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 4.5916 - val_acc: 0.6092\n",
            "Epoch 289/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 4.5045 - val_acc: 0.6129\n",
            "Epoch 290/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 4.5549 - val_acc: 0.6107\n",
            "Epoch 291/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 4.5865 - val_acc: 0.6122\n",
            "Epoch 292/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0016 - acc: 0.9998 - val_loss: 4.6638 - val_acc: 0.6048\n",
            "Epoch 293/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 4.6246 - val_acc: 0.6100\n",
            "Epoch 294/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0578 - acc: 0.9861 - val_loss: 2.6071 - val_acc: 0.5684\n",
            "Epoch 295/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0279 - acc: 0.9931 - val_loss: 2.9845 - val_acc: 0.6218\n",
            "Epoch 296/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0094 - acc: 0.9998 - val_loss: 2.9844 - val_acc: 0.6174\n",
            "Epoch 297/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0082 - acc: 0.9998 - val_loss: 3.1067 - val_acc: 0.6085\n",
            "Epoch 298/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0087 - acc: 0.9994 - val_loss: 3.3086 - val_acc: 0.6233\n",
            "Epoch 299/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0067 - acc: 0.9996 - val_loss: 3.1079 - val_acc: 0.6055\n",
            "Epoch 300/300\n",
            "4809/4809 [==============================] - 12s 3ms/step - loss: 0.0066 - acc: 0.9998 - val_loss: 3.1559 - val_acc: 0.6100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzESnX_NPAtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuk1BYrsJAEv",
        "colab_type": "code",
        "outputId": "2191e463-8f05-44d0-c272-0b5a44ab31bd",
        "colab": {}
      },
      "source": [
        "batch_size=5\n",
        "# prepare data augmentation configuration  . . . do we need this?\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 65 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_GWarpbJAEx",
        "colab_type": "code",
        "outputId": "92bf668d-5c5e-419b-9cba-a4f14f011d50",
        "colab": {}
      },
      "source": [
        "# fine-tune the model\n",
        "model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_samples // batch_size,\n",
        "    epochs=15,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_samples // batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "13/13 [==============================] - 15s 1s/step - loss: 0.6780 - acc: 0.6462 - val_loss: 0.9857 - val_acc: 0.5000\n",
            "Epoch 2/15\n",
            "13/13 [==============================] - 3s 219ms/step - loss: 0.5827 - acc: 0.7077 - val_loss: 0.8727 - val_acc: 0.5000\n",
            "Epoch 3/15\n",
            "13/13 [==============================] - 3s 220ms/step - loss: 0.5248 - acc: 0.7538 - val_loss: 1.0671 - val_acc: 0.5000\n",
            "Epoch 4/15\n",
            "13/13 [==============================] - 3s 211ms/step - loss: 0.4175 - acc: 0.8615 - val_loss: 0.7034 - val_acc: 0.6000\n",
            "Epoch 5/15\n",
            "13/13 [==============================] - 3s 212ms/step - loss: 0.4298 - acc: 0.8462 - val_loss: 0.8096 - val_acc: 0.5000\n",
            "Epoch 6/15\n",
            "13/13 [==============================] - 3s 213ms/step - loss: 0.4825 - acc: 0.8000 - val_loss: 0.4993 - val_acc: 0.7000\n",
            "Epoch 7/15\n",
            "13/13 [==============================] - 3s 212ms/step - loss: 0.4284 - acc: 0.8615 - val_loss: 0.7543 - val_acc: 0.6000\n",
            "Epoch 8/15\n",
            "13/13 [==============================] - 3s 212ms/step - loss: 0.4197 - acc: 0.8154 - val_loss: 0.7259 - val_acc: 0.6000\n",
            "Epoch 9/15\n",
            "13/13 [==============================] - 3s 215ms/step - loss: 0.3696 - acc: 0.8000 - val_loss: 0.8428 - val_acc: 0.5000\n",
            "Epoch 10/15\n",
            "13/13 [==============================] - 3s 216ms/step - loss: 0.4582 - acc: 0.8000 - val_loss: 1.1376 - val_acc: 0.5000\n",
            "Epoch 11/15\n",
            "13/13 [==============================] - 3s 224ms/step - loss: 0.2745 - acc: 0.9077 - val_loss: 1.4454 - val_acc: 0.5000\n",
            "Epoch 12/15\n",
            "13/13 [==============================] - 3s 219ms/step - loss: 0.4227 - acc: 0.8769 - val_loss: 1.2953 - val_acc: 0.5000\n",
            "Epoch 13/15\n",
            "13/13 [==============================] - 3s 217ms/step - loss: 0.2775 - acc: 0.8923 - val_loss: 0.8975 - val_acc: 0.5000\n",
            "Epoch 14/15\n",
            "13/13 [==============================] - 3s 221ms/step - loss: 0.2652 - acc: 0.9077 - val_loss: 1.2739 - val_acc: 0.5000\n",
            "Epoch 15/15\n",
            "13/13 [==============================] - 3s 248ms/step - loss: 0.2288 - acc: 0.9231 - val_loss: 1.1344 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0xb2720ef60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0FV42AcJAEy",
        "colab_type": "code",
        "outputId": "da394ce0-830e-46b7-da25-47cbbe48eb31",
        "colab": {}
      },
      "source": [
        "for i, layer in enumerate(model_v3.layers):\n",
        "    print(i, layer.name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 input_1\n",
            "1 conv2d_1\n",
            "2 batch_normalization_1\n",
            "3 activation_1\n",
            "4 conv2d_2\n",
            "5 batch_normalization_2\n",
            "6 activation_2\n",
            "7 conv2d_3\n",
            "8 batch_normalization_3\n",
            "9 activation_3\n",
            "10 max_pooling2d_1\n",
            "11 conv2d_4\n",
            "12 batch_normalization_4\n",
            "13 activation_4\n",
            "14 conv2d_5\n",
            "15 batch_normalization_5\n",
            "16 activation_5\n",
            "17 max_pooling2d_2\n",
            "18 conv2d_9\n",
            "19 batch_normalization_9\n",
            "20 activation_9\n",
            "21 conv2d_7\n",
            "22 conv2d_10\n",
            "23 batch_normalization_7\n",
            "24 batch_normalization_10\n",
            "25 activation_7\n",
            "26 activation_10\n",
            "27 average_pooling2d_1\n",
            "28 conv2d_6\n",
            "29 conv2d_8\n",
            "30 conv2d_11\n",
            "31 conv2d_12\n",
            "32 batch_normalization_6\n",
            "33 batch_normalization_8\n",
            "34 batch_normalization_11\n",
            "35 batch_normalization_12\n",
            "36 activation_6\n",
            "37 activation_8\n",
            "38 activation_11\n",
            "39 activation_12\n",
            "40 mixed0\n",
            "41 conv2d_16\n",
            "42 batch_normalization_16\n",
            "43 activation_16\n",
            "44 conv2d_14\n",
            "45 conv2d_17\n",
            "46 batch_normalization_14\n",
            "47 batch_normalization_17\n",
            "48 activation_14\n",
            "49 activation_17\n",
            "50 average_pooling2d_2\n",
            "51 conv2d_13\n",
            "52 conv2d_15\n",
            "53 conv2d_18\n",
            "54 conv2d_19\n",
            "55 batch_normalization_13\n",
            "56 batch_normalization_15\n",
            "57 batch_normalization_18\n",
            "58 batch_normalization_19\n",
            "59 activation_13\n",
            "60 activation_15\n",
            "61 activation_18\n",
            "62 activation_19\n",
            "63 mixed1\n",
            "64 conv2d_23\n",
            "65 batch_normalization_23\n",
            "66 activation_23\n",
            "67 conv2d_21\n",
            "68 conv2d_24\n",
            "69 batch_normalization_21\n",
            "70 batch_normalization_24\n",
            "71 activation_21\n",
            "72 activation_24\n",
            "73 average_pooling2d_3\n",
            "74 conv2d_20\n",
            "75 conv2d_22\n",
            "76 conv2d_25\n",
            "77 conv2d_26\n",
            "78 batch_normalization_20\n",
            "79 batch_normalization_22\n",
            "80 batch_normalization_25\n",
            "81 batch_normalization_26\n",
            "82 activation_20\n",
            "83 activation_22\n",
            "84 activation_25\n",
            "85 activation_26\n",
            "86 mixed2\n",
            "87 conv2d_28\n",
            "88 batch_normalization_28\n",
            "89 activation_28\n",
            "90 conv2d_29\n",
            "91 batch_normalization_29\n",
            "92 activation_29\n",
            "93 conv2d_27\n",
            "94 conv2d_30\n",
            "95 batch_normalization_27\n",
            "96 batch_normalization_30\n",
            "97 activation_27\n",
            "98 activation_30\n",
            "99 max_pooling2d_3\n",
            "100 mixed3\n",
            "101 conv2d_35\n",
            "102 batch_normalization_35\n",
            "103 activation_35\n",
            "104 conv2d_36\n",
            "105 batch_normalization_36\n",
            "106 activation_36\n",
            "107 conv2d_32\n",
            "108 conv2d_37\n",
            "109 batch_normalization_32\n",
            "110 batch_normalization_37\n",
            "111 activation_32\n",
            "112 activation_37\n",
            "113 conv2d_33\n",
            "114 conv2d_38\n",
            "115 batch_normalization_33\n",
            "116 batch_normalization_38\n",
            "117 activation_33\n",
            "118 activation_38\n",
            "119 average_pooling2d_4\n",
            "120 conv2d_31\n",
            "121 conv2d_34\n",
            "122 conv2d_39\n",
            "123 conv2d_40\n",
            "124 batch_normalization_31\n",
            "125 batch_normalization_34\n",
            "126 batch_normalization_39\n",
            "127 batch_normalization_40\n",
            "128 activation_31\n",
            "129 activation_34\n",
            "130 activation_39\n",
            "131 activation_40\n",
            "132 mixed4\n",
            "133 conv2d_45\n",
            "134 batch_normalization_45\n",
            "135 activation_45\n",
            "136 conv2d_46\n",
            "137 batch_normalization_46\n",
            "138 activation_46\n",
            "139 conv2d_42\n",
            "140 conv2d_47\n",
            "141 batch_normalization_42\n",
            "142 batch_normalization_47\n",
            "143 activation_42\n",
            "144 activation_47\n",
            "145 conv2d_43\n",
            "146 conv2d_48\n",
            "147 batch_normalization_43\n",
            "148 batch_normalization_48\n",
            "149 activation_43\n",
            "150 activation_48\n",
            "151 average_pooling2d_5\n",
            "152 conv2d_41\n",
            "153 conv2d_44\n",
            "154 conv2d_49\n",
            "155 conv2d_50\n",
            "156 batch_normalization_41\n",
            "157 batch_normalization_44\n",
            "158 batch_normalization_49\n",
            "159 batch_normalization_50\n",
            "160 activation_41\n",
            "161 activation_44\n",
            "162 activation_49\n",
            "163 activation_50\n",
            "164 mixed5\n",
            "165 conv2d_55\n",
            "166 batch_normalization_55\n",
            "167 activation_55\n",
            "168 conv2d_56\n",
            "169 batch_normalization_56\n",
            "170 activation_56\n",
            "171 conv2d_52\n",
            "172 conv2d_57\n",
            "173 batch_normalization_52\n",
            "174 batch_normalization_57\n",
            "175 activation_52\n",
            "176 activation_57\n",
            "177 conv2d_53\n",
            "178 conv2d_58\n",
            "179 batch_normalization_53\n",
            "180 batch_normalization_58\n",
            "181 activation_53\n",
            "182 activation_58\n",
            "183 average_pooling2d_6\n",
            "184 conv2d_51\n",
            "185 conv2d_54\n",
            "186 conv2d_59\n",
            "187 conv2d_60\n",
            "188 batch_normalization_51\n",
            "189 batch_normalization_54\n",
            "190 batch_normalization_59\n",
            "191 batch_normalization_60\n",
            "192 activation_51\n",
            "193 activation_54\n",
            "194 activation_59\n",
            "195 activation_60\n",
            "196 mixed6\n",
            "197 conv2d_65\n",
            "198 batch_normalization_65\n",
            "199 activation_65\n",
            "200 conv2d_66\n",
            "201 batch_normalization_66\n",
            "202 activation_66\n",
            "203 conv2d_62\n",
            "204 conv2d_67\n",
            "205 batch_normalization_62\n",
            "206 batch_normalization_67\n",
            "207 activation_62\n",
            "208 activation_67\n",
            "209 conv2d_63\n",
            "210 conv2d_68\n",
            "211 batch_normalization_63\n",
            "212 batch_normalization_68\n",
            "213 activation_63\n",
            "214 activation_68\n",
            "215 average_pooling2d_7\n",
            "216 conv2d_61\n",
            "217 conv2d_64\n",
            "218 conv2d_69\n",
            "219 conv2d_70\n",
            "220 batch_normalization_61\n",
            "221 batch_normalization_64\n",
            "222 batch_normalization_69\n",
            "223 batch_normalization_70\n",
            "224 activation_61\n",
            "225 activation_64\n",
            "226 activation_69\n",
            "227 activation_70\n",
            "228 mixed7\n",
            "229 conv2d_73\n",
            "230 batch_normalization_73\n",
            "231 activation_73\n",
            "232 conv2d_74\n",
            "233 batch_normalization_74\n",
            "234 activation_74\n",
            "235 conv2d_71\n",
            "236 conv2d_75\n",
            "237 batch_normalization_71\n",
            "238 batch_normalization_75\n",
            "239 activation_71\n",
            "240 activation_75\n",
            "241 conv2d_72\n",
            "242 conv2d_76\n",
            "243 batch_normalization_72\n",
            "244 batch_normalization_76\n",
            "245 activation_72\n",
            "246 activation_76\n",
            "247 max_pooling2d_4\n",
            "248 mixed8\n",
            "249 conv2d_81\n",
            "250 batch_normalization_81\n",
            "251 activation_81\n",
            "252 conv2d_78\n",
            "253 conv2d_82\n",
            "254 batch_normalization_78\n",
            "255 batch_normalization_82\n",
            "256 activation_78\n",
            "257 activation_82\n",
            "258 conv2d_79\n",
            "259 conv2d_80\n",
            "260 conv2d_83\n",
            "261 conv2d_84\n",
            "262 average_pooling2d_8\n",
            "263 conv2d_77\n",
            "264 batch_normalization_79\n",
            "265 batch_normalization_80\n",
            "266 batch_normalization_83\n",
            "267 batch_normalization_84\n",
            "268 conv2d_85\n",
            "269 batch_normalization_77\n",
            "270 activation_79\n",
            "271 activation_80\n",
            "272 activation_83\n",
            "273 activation_84\n",
            "274 batch_normalization_85\n",
            "275 activation_77\n",
            "276 mixed9_0\n",
            "277 concatenate_1\n",
            "278 activation_85\n",
            "279 mixed9\n",
            "280 conv2d_90\n",
            "281 batch_normalization_90\n",
            "282 activation_90\n",
            "283 conv2d_87\n",
            "284 conv2d_91\n",
            "285 batch_normalization_87\n",
            "286 batch_normalization_91\n",
            "287 activation_87\n",
            "288 activation_91\n",
            "289 conv2d_88\n",
            "290 conv2d_89\n",
            "291 conv2d_92\n",
            "292 conv2d_93\n",
            "293 average_pooling2d_9\n",
            "294 conv2d_86\n",
            "295 batch_normalization_88\n",
            "296 batch_normalization_89\n",
            "297 batch_normalization_92\n",
            "298 batch_normalization_93\n",
            "299 conv2d_94\n",
            "300 batch_normalization_86\n",
            "301 activation_88\n",
            "302 activation_89\n",
            "303 activation_92\n",
            "304 activation_93\n",
            "305 batch_normalization_94\n",
            "306 activation_86\n",
            "307 mixed9_1\n",
            "308 concatenate_2\n",
            "309 activation_94\n",
            "310 mixed10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT6JNlLVJAE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in model.layers[:249]:\n",
        "    layer.trainable = False\n",
        "for layer in model.layers[249:]:\n",
        "    layer.trainable = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LySfZPS8JAE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD\n",
        "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT4LphghJAE5",
        "colab_type": "code",
        "outputId": "f87a5aa2-e48f-4912-bd42-b57d23d279b0",
        "colab": {}
      },
      "source": [
        "model.fit_generator(train_generator,\n",
        "    steps_per_epoch=train_samples // batch_size,\n",
        "    epochs=15,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_samples // batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "13/13 [==============================] - 20s 2s/step - loss: 0.3586 - acc: 0.8462 - val_loss: 0.3201 - val_acc: 0.9000\n",
            "Epoch 2/15\n",
            "13/13 [==============================] - 4s 296ms/step - loss: 0.2870 - acc: 0.8769 - val_loss: 0.2570 - val_acc: 0.9000\n",
            "Epoch 3/15\n",
            "13/13 [==============================] - 4s 295ms/step - loss: 0.2343 - acc: 0.9538 - val_loss: 0.4785 - val_acc: 0.7000\n",
            "Epoch 4/15\n",
            "13/13 [==============================] - 4s 295ms/step - loss: 0.1493 - acc: 1.0000 - val_loss: 0.3594 - val_acc: 0.8000\n",
            "Epoch 5/15\n",
            "13/13 [==============================] - 4s 303ms/step - loss: 0.3103 - acc: 0.9077 - val_loss: 0.4878 - val_acc: 0.7000\n",
            "Epoch 6/15\n",
            "13/13 [==============================] - 4s 308ms/step - loss: 0.1814 - acc: 0.9538 - val_loss: 0.2861 - val_acc: 0.9000\n",
            "Epoch 7/15\n",
            "13/13 [==============================] - 4s 313ms/step - loss: 0.2428 - acc: 0.9077 - val_loss: 0.3010 - val_acc: 0.8000\n",
            "Epoch 8/15\n",
            "13/13 [==============================] - 4s 328ms/step - loss: 0.3597 - acc: 0.8923 - val_loss: 0.4252 - val_acc: 0.8000\n",
            "Epoch 9/15\n",
            "13/13 [==============================] - 4s 314ms/step - loss: 0.3290 - acc: 0.8462 - val_loss: 0.4314 - val_acc: 0.7000\n",
            "Epoch 10/15\n",
            "13/13 [==============================] - 4s 320ms/step - loss: 0.2718 - acc: 0.8923 - val_loss: 0.2478 - val_acc: 0.9000\n",
            "Epoch 11/15\n",
            "13/13 [==============================] - 4s 319ms/step - loss: 0.2319 - acc: 0.8923 - val_loss: 0.4928 - val_acc: 0.8000\n",
            "Epoch 12/15\n",
            "13/13 [==============================] - 4s 322ms/step - loss: 0.2656 - acc: 0.9231 - val_loss: 0.6266 - val_acc: 0.7000\n",
            "Epoch 13/15\n",
            "13/13 [==============================] - 4s 346ms/step - loss: 0.3549 - acc: 0.8462 - val_loss: 0.2696 - val_acc: 0.9000\n",
            "Epoch 14/15\n",
            "13/13 [==============================] - 4s 341ms/step - loss: 0.2751 - acc: 0.8615 - val_loss: 0.3961 - val_acc: 0.8000\n",
            "Epoch 15/15\n",
            "13/13 [==============================] - 4s 341ms/step - loss: 0.2004 - acc: 0.9538 - val_loss: 0.5810 - val_acc: 0.7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0xb2d9a1828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAmEqe-OJAE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('models/finetuning_30epochs_vgg.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Qg3XYQPJAE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('models/finetuning_30epochs_vgg.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMURHESwJAE_",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating on validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "835Wsvh8JAFA",
        "colab_type": "text"
      },
      "source": [
        "Computing loss and accuracy :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qAkuFBqJAFA",
        "colab_type": "code",
        "outputId": "1aedcb34-cd80-44e9-b8ac-aa977d23191d",
        "colab": {}
      },
      "source": [
        "model.evaluate_generator(validation_generator, validation_samples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33350689638357905, 0.93280498798076927]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}